{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "from args import get_parser\n",
    "import pickle\n",
    "from model import get_model\n",
    "from torchvision import transforms\n",
    "from utils.output_utils import prepare_output\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.encoder import EncoderVisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code will run in gpu if available and if the flag is set to True, else it will run on cpu\n",
    "use_gpu = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and use_gpu else 'cpu')\n",
    "map_loc = None if torch.cuda.is_available() and use_gpu else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingrs_vocab = pickle.load(open('/data/prateek/github/see-food/garbage/ingr_vocab.pkl', 'rb'))\n",
    "vocab = pickle.load(open('/data/prateek/github/see-food/garbage/instr_vocab.pkl', 'rb'))\n",
    "\n",
    "\n",
    "ingr_vocab_size = len(ingrs_vocab)\n",
    "instrs_vocab_size = len(vocab)\n",
    "output_dim = instrs_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4660 512\n"
     ]
    }
   ],
   "source": [
    "print(instrs_vocab_size, ingr_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "import sys; sys.argv=['']; del sys\n",
    "args = get_parser()\n",
    "args.maxseqlen = 15\n",
    "\n",
    "# args.ingrs_only=False\n",
    "args.use_vision_transformer=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using vision: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/nas/home/pchhikar/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model = get_model(args, ingr_vocab_size, instrs_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <class 'model.InverseCookingModel'>\n",
      "ingredient_encoder <class 'modules.encoder.EncoderLabels'>\n",
      "ingredient_encoder.linear <class 'torch.nn.modules.sparse.Embedding'>\n",
      "recipe_decoder <class 'modules.transformer_decoder.DecoderTransformer'>\n",
      "recipe_decoder.embed_tokens <class 'torch.nn.modules.sparse.Embedding'>\n",
      "recipe_decoder.embed_positions <class 'modules.transformer_decoder.LearnedPositionalEmbedding'>\n",
      "recipe_decoder.layers <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.0 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "recipe_decoder.layers.0.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.0.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.0.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.0.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.0.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.0.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.0.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.0.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.0.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.0.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.1 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "recipe_decoder.layers.1.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.1.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.1.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.1.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.1.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.1.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.1.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.1.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.1.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.1.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.2 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "recipe_decoder.layers.2.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.2.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.2.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.2.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.2.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.2.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.2.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.2.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.2.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.2.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.3 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "recipe_decoder.layers.3.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.3.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.3.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.3.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.3.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.3.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.3.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.3.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.3.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.3.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.4 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "recipe_decoder.layers.4.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.4.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.4.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.4.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.4.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.4.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.4.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.4.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.4.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.4.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.5 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "recipe_decoder.layers.5.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.5.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.5.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.5.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.5.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.5.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.5.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.5.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.5.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.5.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.6 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "recipe_decoder.layers.6.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.6.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.6.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.6.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.6.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.6.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.6.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.6.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.6.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.6.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.7 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "recipe_decoder.layers.7.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.7.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.7.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.7.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.7.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.7.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.7.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.7.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.7.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.7.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.8 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "recipe_decoder.layers.8.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.8.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.8.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.8.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.8.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.8.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.8.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.8.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.8.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.8.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.9 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "recipe_decoder.layers.9.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.9.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.9.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.9.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.9.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.9.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.9.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.9.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.9.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.9.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.10 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "recipe_decoder.layers.10.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.10.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.10.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.10.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.10.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.10.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.10.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.10.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.10.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.10.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.11 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "recipe_decoder.layers.11.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.11.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.11.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.11.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.11.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.11.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.11.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.11.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.11.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.11.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.12 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "recipe_decoder.layers.12.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.12.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.12.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.12.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.12.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.12.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.12.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.12.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.12.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.12.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.13 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "recipe_decoder.layers.13.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.13.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.13.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.13.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.13.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.13.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.13.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.13.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.13.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.13.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.14 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "recipe_decoder.layers.14.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.14.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.14.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.14.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.14.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.14.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.14.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.14.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.14.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.14.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.15 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "recipe_decoder.layers.15.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.15.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.15.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "recipe_decoder.layers.15.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.15.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.15.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "recipe_decoder.layers.15.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "recipe_decoder.layers.15.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.15.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.layers.15.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "recipe_decoder.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder <class 'modules.encoder.EncoderVisionTransformer'>\n",
      "image_encoder.vit <class 'transformers.models.vit.modeling_vit.ViTModel'>\n",
      "image_encoder.vit.embeddings <class 'transformers.models.vit.modeling_vit.ViTEmbeddings'>\n",
      "image_encoder.vit.embeddings.patch_embeddings <class 'transformers.models.vit.modeling_vit.ViTPatchEmbeddings'>\n",
      "image_encoder.vit.embeddings.patch_embeddings.projection <class 'torch.nn.modules.conv.Conv2d'>\n",
      "image_encoder.vit.embeddings.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder <class 'transformers.models.vit.modeling_vit.ViTEncoder'>\n",
      "image_encoder.vit.encoder.layer <class 'torch.nn.modules.container.ModuleList'>\n",
      "image_encoder.vit.encoder.layer.0 <class 'transformers.models.vit.modeling_vit.ViTLayer'>\n",
      "image_encoder.vit.encoder.layer.0.attention <class 'transformers.models.vit.modeling_vit.ViTAttention'>\n",
      "image_encoder.vit.encoder.layer.0.attention.attention <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'>\n",
      "image_encoder.vit.encoder.layer.0.attention.attention.query <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.0.attention.attention.key <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.0.attention.attention.value <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.0.attention.attention.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.0.attention.output <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'>\n",
      "image_encoder.vit.encoder.layer.0.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.0.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.0.intermediate <class 'transformers.models.vit.modeling_vit.ViTIntermediate'>\n",
      "image_encoder.vit.encoder.layer.0.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.0.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "image_encoder.vit.encoder.layer.0.output <class 'transformers.models.vit.modeling_vit.ViTOutput'>\n",
      "image_encoder.vit.encoder.layer.0.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.0.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.0.layernorm_before <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.0.layernorm_after <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.1 <class 'transformers.models.vit.modeling_vit.ViTLayer'>\n",
      "image_encoder.vit.encoder.layer.1.attention <class 'transformers.models.vit.modeling_vit.ViTAttention'>\n",
      "image_encoder.vit.encoder.layer.1.attention.attention <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'>\n",
      "image_encoder.vit.encoder.layer.1.attention.attention.query <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.1.attention.attention.key <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.1.attention.attention.value <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.1.attention.attention.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.1.attention.output <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'>\n",
      "image_encoder.vit.encoder.layer.1.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.1.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.1.intermediate <class 'transformers.models.vit.modeling_vit.ViTIntermediate'>\n",
      "image_encoder.vit.encoder.layer.1.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.1.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "image_encoder.vit.encoder.layer.1.output <class 'transformers.models.vit.modeling_vit.ViTOutput'>\n",
      "image_encoder.vit.encoder.layer.1.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.1.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.1.layernorm_before <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.1.layernorm_after <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.2 <class 'transformers.models.vit.modeling_vit.ViTLayer'>\n",
      "image_encoder.vit.encoder.layer.2.attention <class 'transformers.models.vit.modeling_vit.ViTAttention'>\n",
      "image_encoder.vit.encoder.layer.2.attention.attention <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'>\n",
      "image_encoder.vit.encoder.layer.2.attention.attention.query <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.2.attention.attention.key <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.2.attention.attention.value <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.2.attention.attention.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.2.attention.output <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'>\n",
      "image_encoder.vit.encoder.layer.2.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.2.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.2.intermediate <class 'transformers.models.vit.modeling_vit.ViTIntermediate'>\n",
      "image_encoder.vit.encoder.layer.2.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.2.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "image_encoder.vit.encoder.layer.2.output <class 'transformers.models.vit.modeling_vit.ViTOutput'>\n",
      "image_encoder.vit.encoder.layer.2.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.2.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.2.layernorm_before <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.2.layernorm_after <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.3 <class 'transformers.models.vit.modeling_vit.ViTLayer'>\n",
      "image_encoder.vit.encoder.layer.3.attention <class 'transformers.models.vit.modeling_vit.ViTAttention'>\n",
      "image_encoder.vit.encoder.layer.3.attention.attention <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'>\n",
      "image_encoder.vit.encoder.layer.3.attention.attention.query <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.3.attention.attention.key <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.3.attention.attention.value <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.3.attention.attention.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.3.attention.output <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'>\n",
      "image_encoder.vit.encoder.layer.3.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.3.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.3.intermediate <class 'transformers.models.vit.modeling_vit.ViTIntermediate'>\n",
      "image_encoder.vit.encoder.layer.3.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.3.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "image_encoder.vit.encoder.layer.3.output <class 'transformers.models.vit.modeling_vit.ViTOutput'>\n",
      "image_encoder.vit.encoder.layer.3.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.3.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.3.layernorm_before <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.3.layernorm_after <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.4 <class 'transformers.models.vit.modeling_vit.ViTLayer'>\n",
      "image_encoder.vit.encoder.layer.4.attention <class 'transformers.models.vit.modeling_vit.ViTAttention'>\n",
      "image_encoder.vit.encoder.layer.4.attention.attention <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'>\n",
      "image_encoder.vit.encoder.layer.4.attention.attention.query <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.4.attention.attention.key <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.4.attention.attention.value <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.4.attention.attention.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.4.attention.output <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'>\n",
      "image_encoder.vit.encoder.layer.4.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.4.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.4.intermediate <class 'transformers.models.vit.modeling_vit.ViTIntermediate'>\n",
      "image_encoder.vit.encoder.layer.4.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.4.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "image_encoder.vit.encoder.layer.4.output <class 'transformers.models.vit.modeling_vit.ViTOutput'>\n",
      "image_encoder.vit.encoder.layer.4.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.4.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.4.layernorm_before <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.4.layernorm_after <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.5 <class 'transformers.models.vit.modeling_vit.ViTLayer'>\n",
      "image_encoder.vit.encoder.layer.5.attention <class 'transformers.models.vit.modeling_vit.ViTAttention'>\n",
      "image_encoder.vit.encoder.layer.5.attention.attention <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'>\n",
      "image_encoder.vit.encoder.layer.5.attention.attention.query <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.5.attention.attention.key <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.5.attention.attention.value <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.5.attention.attention.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.5.attention.output <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'>\n",
      "image_encoder.vit.encoder.layer.5.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.5.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.5.intermediate <class 'transformers.models.vit.modeling_vit.ViTIntermediate'>\n",
      "image_encoder.vit.encoder.layer.5.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.5.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "image_encoder.vit.encoder.layer.5.output <class 'transformers.models.vit.modeling_vit.ViTOutput'>\n",
      "image_encoder.vit.encoder.layer.5.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.5.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.5.layernorm_before <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.5.layernorm_after <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.6 <class 'transformers.models.vit.modeling_vit.ViTLayer'>\n",
      "image_encoder.vit.encoder.layer.6.attention <class 'transformers.models.vit.modeling_vit.ViTAttention'>\n",
      "image_encoder.vit.encoder.layer.6.attention.attention <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'>\n",
      "image_encoder.vit.encoder.layer.6.attention.attention.query <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.6.attention.attention.key <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.6.attention.attention.value <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.6.attention.attention.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.6.attention.output <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'>\n",
      "image_encoder.vit.encoder.layer.6.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.6.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.6.intermediate <class 'transformers.models.vit.modeling_vit.ViTIntermediate'>\n",
      "image_encoder.vit.encoder.layer.6.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.6.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "image_encoder.vit.encoder.layer.6.output <class 'transformers.models.vit.modeling_vit.ViTOutput'>\n",
      "image_encoder.vit.encoder.layer.6.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.6.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.6.layernorm_before <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.6.layernorm_after <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.7 <class 'transformers.models.vit.modeling_vit.ViTLayer'>\n",
      "image_encoder.vit.encoder.layer.7.attention <class 'transformers.models.vit.modeling_vit.ViTAttention'>\n",
      "image_encoder.vit.encoder.layer.7.attention.attention <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'>\n",
      "image_encoder.vit.encoder.layer.7.attention.attention.query <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.7.attention.attention.key <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.7.attention.attention.value <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.7.attention.attention.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.7.attention.output <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'>\n",
      "image_encoder.vit.encoder.layer.7.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.7.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.7.intermediate <class 'transformers.models.vit.modeling_vit.ViTIntermediate'>\n",
      "image_encoder.vit.encoder.layer.7.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.7.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "image_encoder.vit.encoder.layer.7.output <class 'transformers.models.vit.modeling_vit.ViTOutput'>\n",
      "image_encoder.vit.encoder.layer.7.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.7.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.7.layernorm_before <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.7.layernorm_after <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.8 <class 'transformers.models.vit.modeling_vit.ViTLayer'>\n",
      "image_encoder.vit.encoder.layer.8.attention <class 'transformers.models.vit.modeling_vit.ViTAttention'>\n",
      "image_encoder.vit.encoder.layer.8.attention.attention <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'>\n",
      "image_encoder.vit.encoder.layer.8.attention.attention.query <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.8.attention.attention.key <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.8.attention.attention.value <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.8.attention.attention.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.8.attention.output <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'>\n",
      "image_encoder.vit.encoder.layer.8.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.8.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.8.intermediate <class 'transformers.models.vit.modeling_vit.ViTIntermediate'>\n",
      "image_encoder.vit.encoder.layer.8.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.8.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "image_encoder.vit.encoder.layer.8.output <class 'transformers.models.vit.modeling_vit.ViTOutput'>\n",
      "image_encoder.vit.encoder.layer.8.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.8.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.8.layernorm_before <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.8.layernorm_after <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.9 <class 'transformers.models.vit.modeling_vit.ViTLayer'>\n",
      "image_encoder.vit.encoder.layer.9.attention <class 'transformers.models.vit.modeling_vit.ViTAttention'>\n",
      "image_encoder.vit.encoder.layer.9.attention.attention <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'>\n",
      "image_encoder.vit.encoder.layer.9.attention.attention.query <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.9.attention.attention.key <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.9.attention.attention.value <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.9.attention.attention.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.9.attention.output <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'>\n",
      "image_encoder.vit.encoder.layer.9.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.9.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.9.intermediate <class 'transformers.models.vit.modeling_vit.ViTIntermediate'>\n",
      "image_encoder.vit.encoder.layer.9.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.9.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "image_encoder.vit.encoder.layer.9.output <class 'transformers.models.vit.modeling_vit.ViTOutput'>\n",
      "image_encoder.vit.encoder.layer.9.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.9.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.9.layernorm_before <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.9.layernorm_after <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.10 <class 'transformers.models.vit.modeling_vit.ViTLayer'>\n",
      "image_encoder.vit.encoder.layer.10.attention <class 'transformers.models.vit.modeling_vit.ViTAttention'>\n",
      "image_encoder.vit.encoder.layer.10.attention.attention <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'>\n",
      "image_encoder.vit.encoder.layer.10.attention.attention.query <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.10.attention.attention.key <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.10.attention.attention.value <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.10.attention.attention.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.10.attention.output <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'>\n",
      "image_encoder.vit.encoder.layer.10.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.10.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.10.intermediate <class 'transformers.models.vit.modeling_vit.ViTIntermediate'>\n",
      "image_encoder.vit.encoder.layer.10.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.10.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "image_encoder.vit.encoder.layer.10.output <class 'transformers.models.vit.modeling_vit.ViTOutput'>\n",
      "image_encoder.vit.encoder.layer.10.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.10.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.10.layernorm_before <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.10.layernorm_after <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.11 <class 'transformers.models.vit.modeling_vit.ViTLayer'>\n",
      "image_encoder.vit.encoder.layer.11.attention <class 'transformers.models.vit.modeling_vit.ViTAttention'>\n",
      "image_encoder.vit.encoder.layer.11.attention.attention <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'>\n",
      "image_encoder.vit.encoder.layer.11.attention.attention.query <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.11.attention.attention.key <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.11.attention.attention.value <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.11.attention.attention.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.11.attention.output <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'>\n",
      "image_encoder.vit.encoder.layer.11.attention.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.11.attention.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.11.intermediate <class 'transformers.models.vit.modeling_vit.ViTIntermediate'>\n",
      "image_encoder.vit.encoder.layer.11.intermediate.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.11.intermediate.intermediate_act_fn <class 'transformers.activations.GELUActivation'>\n",
      "image_encoder.vit.encoder.layer.11.output <class 'transformers.models.vit.modeling_vit.ViTOutput'>\n",
      "image_encoder.vit.encoder.layer.11.output.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.encoder.layer.11.output.dropout <class 'torch.nn.modules.dropout.Dropout'>\n",
      "image_encoder.vit.encoder.layer.11.layernorm_before <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.encoder.layer.11.layernorm_after <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.layernorm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "image_encoder.vit.pooler <class 'transformers.models.vit.modeling_vit.ViTPooler'>\n",
      "image_encoder.vit.pooler.dense <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.vit.pooler.activation <class 'torch.nn.modules.activation.Tanh'>\n",
      "image_encoder.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "image_encoder.sequential <class 'torch.nn.modules.container.Sequential'>\n",
      "image_encoder.sequential.0 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "image_encoder.sequential.1 <class 'torch.nn.modules.dropout.Dropout2d'>\n",
      "ingredient_decoder <class 'modules.transformer_decoder.DecoderTransformer'>\n",
      "ingredient_decoder.embed_tokens <class 'torch.nn.modules.sparse.Embedding'>\n",
      "ingredient_decoder.layer_norms_in <class 'torch.nn.modules.container.ModuleList'>\n",
      "ingredient_decoder.layer_norms_in.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layer_norms_in.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layer_norms_in.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layers <class 'torch.nn.modules.container.ModuleList'>\n",
      "ingredient_decoder.layers.0 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "ingredient_decoder.layers.0.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "ingredient_decoder.layers.0.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "ingredient_decoder.layers.0.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "ingredient_decoder.layers.0.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "ingredient_decoder.layers.0.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "ingredient_decoder.layers.0.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "ingredient_decoder.layers.0.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "ingredient_decoder.layers.0.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layers.0.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layers.0.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layers.0.last_ln <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layers.1 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "ingredient_decoder.layers.1.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "ingredient_decoder.layers.1.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "ingredient_decoder.layers.1.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "ingredient_decoder.layers.1.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "ingredient_decoder.layers.1.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "ingredient_decoder.layers.1.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "ingredient_decoder.layers.1.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "ingredient_decoder.layers.1.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layers.1.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layers.1.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layers.1.last_ln <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layers.2 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "ingredient_decoder.layers.2.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "ingredient_decoder.layers.2.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "ingredient_decoder.layers.2.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "ingredient_decoder.layers.2.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "ingredient_decoder.layers.2.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "ingredient_decoder.layers.2.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "ingredient_decoder.layers.2.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "ingredient_decoder.layers.2.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layers.2.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layers.2.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layers.2.last_ln <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layers.3 <class 'modules.transformer_decoder.TransformerDecoderLayer'>\n",
      "ingredient_decoder.layers.3.self_attn <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "ingredient_decoder.layers.3.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "ingredient_decoder.layers.3.cond_att <class 'modules.multihead_attention.MultiheadAttention'>\n",
      "ingredient_decoder.layers.3.cond_att.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "ingredient_decoder.layers.3.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "ingredient_decoder.layers.3.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "ingredient_decoder.layers.3.layer_norms <class 'torch.nn.modules.container.ModuleList'>\n",
      "ingredient_decoder.layers.3.layer_norms.0 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layers.3.layer_norms.1 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layers.3.layer_norms.2 <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.layers.3.last_ln <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "ingredient_decoder.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "crit <class 'utils.metrics.MaskedCrossEntropyCriterion'>\n",
      "crit_ingr <class 'torch.nn.modules.loss.BCELoss'>\n",
      "crit_eos <class 'torch.nn.modules.loss.BCELoss'>\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name, type(module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model.image_encoder = EncoderVisionTransformer(args.embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingredient_encoder.linear.weight\n",
      "recipe_decoder.embed_tokens.weight\n",
      "recipe_decoder.embed_positions.weight\n",
      "recipe_decoder.layers.0.self_attn.in_proj_weight\n",
      "recipe_decoder.layers.0.self_attn.in_proj_bias\n",
      "recipe_decoder.layers.0.self_attn.out_proj.weight\n",
      "recipe_decoder.layers.0.self_attn.out_proj.bias\n",
      "recipe_decoder.layers.0.cond_att.in_proj_weight\n",
      "recipe_decoder.layers.0.cond_att.in_proj_bias\n",
      "recipe_decoder.layers.0.cond_att.out_proj.weight\n",
      "recipe_decoder.layers.0.cond_att.out_proj.bias\n",
      "recipe_decoder.layers.0.fc1.weight\n",
      "recipe_decoder.layers.0.fc1.bias\n",
      "recipe_decoder.layers.0.fc2.weight\n",
      "recipe_decoder.layers.0.fc2.bias\n",
      "recipe_decoder.layers.0.layer_norms.0.weight\n",
      "recipe_decoder.layers.0.layer_norms.0.bias\n",
      "recipe_decoder.layers.0.layer_norms.1.weight\n",
      "recipe_decoder.layers.0.layer_norms.1.bias\n",
      "recipe_decoder.layers.0.layer_norms.2.weight\n",
      "recipe_decoder.layers.0.layer_norms.2.bias\n",
      "recipe_decoder.layers.1.self_attn.in_proj_weight\n",
      "recipe_decoder.layers.1.self_attn.in_proj_bias\n",
      "recipe_decoder.layers.1.self_attn.out_proj.weight\n",
      "recipe_decoder.layers.1.self_attn.out_proj.bias\n",
      "recipe_decoder.layers.1.cond_att.in_proj_weight\n",
      "recipe_decoder.layers.1.cond_att.in_proj_bias\n",
      "recipe_decoder.layers.1.cond_att.out_proj.weight\n",
      "recipe_decoder.layers.1.cond_att.out_proj.bias\n",
      "recipe_decoder.layers.1.fc1.weight\n",
      "recipe_decoder.layers.1.fc1.bias\n",
      "recipe_decoder.layers.1.fc2.weight\n",
      "recipe_decoder.layers.1.fc2.bias\n",
      "recipe_decoder.layers.1.layer_norms.0.weight\n",
      "recipe_decoder.layers.1.layer_norms.0.bias\n",
      "recipe_decoder.layers.1.layer_norms.1.weight\n",
      "recipe_decoder.layers.1.layer_norms.1.bias\n",
      "recipe_decoder.layers.1.layer_norms.2.weight\n",
      "recipe_decoder.layers.1.layer_norms.2.bias\n",
      "recipe_decoder.layers.2.self_attn.in_proj_weight\n",
      "recipe_decoder.layers.2.self_attn.in_proj_bias\n",
      "recipe_decoder.layers.2.self_attn.out_proj.weight\n",
      "recipe_decoder.layers.2.self_attn.out_proj.bias\n",
      "recipe_decoder.layers.2.cond_att.in_proj_weight\n",
      "recipe_decoder.layers.2.cond_att.in_proj_bias\n",
      "recipe_decoder.layers.2.cond_att.out_proj.weight\n",
      "recipe_decoder.layers.2.cond_att.out_proj.bias\n",
      "recipe_decoder.layers.2.fc1.weight\n",
      "recipe_decoder.layers.2.fc1.bias\n",
      "recipe_decoder.layers.2.fc2.weight\n",
      "recipe_decoder.layers.2.fc2.bias\n",
      "recipe_decoder.layers.2.layer_norms.0.weight\n",
      "recipe_decoder.layers.2.layer_norms.0.bias\n",
      "recipe_decoder.layers.2.layer_norms.1.weight\n",
      "recipe_decoder.layers.2.layer_norms.1.bias\n",
      "recipe_decoder.layers.2.layer_norms.2.weight\n",
      "recipe_decoder.layers.2.layer_norms.2.bias\n",
      "recipe_decoder.layers.3.self_attn.in_proj_weight\n",
      "recipe_decoder.layers.3.self_attn.in_proj_bias\n",
      "recipe_decoder.layers.3.self_attn.out_proj.weight\n",
      "recipe_decoder.layers.3.self_attn.out_proj.bias\n",
      "recipe_decoder.layers.3.cond_att.in_proj_weight\n",
      "recipe_decoder.layers.3.cond_att.in_proj_bias\n",
      "recipe_decoder.layers.3.cond_att.out_proj.weight\n",
      "recipe_decoder.layers.3.cond_att.out_proj.bias\n",
      "recipe_decoder.layers.3.fc1.weight\n",
      "recipe_decoder.layers.3.fc1.bias\n",
      "recipe_decoder.layers.3.fc2.weight\n",
      "recipe_decoder.layers.3.fc2.bias\n",
      "recipe_decoder.layers.3.layer_norms.0.weight\n",
      "recipe_decoder.layers.3.layer_norms.0.bias\n",
      "recipe_decoder.layers.3.layer_norms.1.weight\n",
      "recipe_decoder.layers.3.layer_norms.1.bias\n",
      "recipe_decoder.layers.3.layer_norms.2.weight\n",
      "recipe_decoder.layers.3.layer_norms.2.bias\n",
      "recipe_decoder.layers.4.self_attn.in_proj_weight\n",
      "recipe_decoder.layers.4.self_attn.in_proj_bias\n",
      "recipe_decoder.layers.4.self_attn.out_proj.weight\n",
      "recipe_decoder.layers.4.self_attn.out_proj.bias\n",
      "recipe_decoder.layers.4.cond_att.in_proj_weight\n",
      "recipe_decoder.layers.4.cond_att.in_proj_bias\n",
      "recipe_decoder.layers.4.cond_att.out_proj.weight\n",
      "recipe_decoder.layers.4.cond_att.out_proj.bias\n",
      "recipe_decoder.layers.4.fc1.weight\n",
      "recipe_decoder.layers.4.fc1.bias\n",
      "recipe_decoder.layers.4.fc2.weight\n",
      "recipe_decoder.layers.4.fc2.bias\n",
      "recipe_decoder.layers.4.layer_norms.0.weight\n",
      "recipe_decoder.layers.4.layer_norms.0.bias\n",
      "recipe_decoder.layers.4.layer_norms.1.weight\n",
      "recipe_decoder.layers.4.layer_norms.1.bias\n",
      "recipe_decoder.layers.4.layer_norms.2.weight\n",
      "recipe_decoder.layers.4.layer_norms.2.bias\n",
      "recipe_decoder.layers.5.self_attn.in_proj_weight\n",
      "recipe_decoder.layers.5.self_attn.in_proj_bias\n",
      "recipe_decoder.layers.5.self_attn.out_proj.weight\n",
      "recipe_decoder.layers.5.self_attn.out_proj.bias\n",
      "recipe_decoder.layers.5.cond_att.in_proj_weight\n",
      "recipe_decoder.layers.5.cond_att.in_proj_bias\n",
      "recipe_decoder.layers.5.cond_att.out_proj.weight\n",
      "recipe_decoder.layers.5.cond_att.out_proj.bias\n",
      "recipe_decoder.layers.5.fc1.weight\n",
      "recipe_decoder.layers.5.fc1.bias\n",
      "recipe_decoder.layers.5.fc2.weight\n",
      "recipe_decoder.layers.5.fc2.bias\n",
      "recipe_decoder.layers.5.layer_norms.0.weight\n",
      "recipe_decoder.layers.5.layer_norms.0.bias\n",
      "recipe_decoder.layers.5.layer_norms.1.weight\n",
      "recipe_decoder.layers.5.layer_norms.1.bias\n",
      "recipe_decoder.layers.5.layer_norms.2.weight\n",
      "recipe_decoder.layers.5.layer_norms.2.bias\n",
      "recipe_decoder.layers.6.self_attn.in_proj_weight\n",
      "recipe_decoder.layers.6.self_attn.in_proj_bias\n",
      "recipe_decoder.layers.6.self_attn.out_proj.weight\n",
      "recipe_decoder.layers.6.self_attn.out_proj.bias\n",
      "recipe_decoder.layers.6.cond_att.in_proj_weight\n",
      "recipe_decoder.layers.6.cond_att.in_proj_bias\n",
      "recipe_decoder.layers.6.cond_att.out_proj.weight\n",
      "recipe_decoder.layers.6.cond_att.out_proj.bias\n",
      "recipe_decoder.layers.6.fc1.weight\n",
      "recipe_decoder.layers.6.fc1.bias\n",
      "recipe_decoder.layers.6.fc2.weight\n",
      "recipe_decoder.layers.6.fc2.bias\n",
      "recipe_decoder.layers.6.layer_norms.0.weight\n",
      "recipe_decoder.layers.6.layer_norms.0.bias\n",
      "recipe_decoder.layers.6.layer_norms.1.weight\n",
      "recipe_decoder.layers.6.layer_norms.1.bias\n",
      "recipe_decoder.layers.6.layer_norms.2.weight\n",
      "recipe_decoder.layers.6.layer_norms.2.bias\n",
      "recipe_decoder.layers.7.self_attn.in_proj_weight\n",
      "recipe_decoder.layers.7.self_attn.in_proj_bias\n",
      "recipe_decoder.layers.7.self_attn.out_proj.weight\n",
      "recipe_decoder.layers.7.self_attn.out_proj.bias\n",
      "recipe_decoder.layers.7.cond_att.in_proj_weight\n",
      "recipe_decoder.layers.7.cond_att.in_proj_bias\n",
      "recipe_decoder.layers.7.cond_att.out_proj.weight\n",
      "recipe_decoder.layers.7.cond_att.out_proj.bias\n",
      "recipe_decoder.layers.7.fc1.weight\n",
      "recipe_decoder.layers.7.fc1.bias\n",
      "recipe_decoder.layers.7.fc2.weight\n",
      "recipe_decoder.layers.7.fc2.bias\n",
      "recipe_decoder.layers.7.layer_norms.0.weight\n",
      "recipe_decoder.layers.7.layer_norms.0.bias\n",
      "recipe_decoder.layers.7.layer_norms.1.weight\n",
      "recipe_decoder.layers.7.layer_norms.1.bias\n",
      "recipe_decoder.layers.7.layer_norms.2.weight\n",
      "recipe_decoder.layers.7.layer_norms.2.bias\n",
      "recipe_decoder.layers.8.self_attn.in_proj_weight\n",
      "recipe_decoder.layers.8.self_attn.in_proj_bias\n",
      "recipe_decoder.layers.8.self_attn.out_proj.weight\n",
      "recipe_decoder.layers.8.self_attn.out_proj.bias\n",
      "recipe_decoder.layers.8.cond_att.in_proj_weight\n",
      "recipe_decoder.layers.8.cond_att.in_proj_bias\n",
      "recipe_decoder.layers.8.cond_att.out_proj.weight\n",
      "recipe_decoder.layers.8.cond_att.out_proj.bias\n",
      "recipe_decoder.layers.8.fc1.weight\n",
      "recipe_decoder.layers.8.fc1.bias\n",
      "recipe_decoder.layers.8.fc2.weight\n",
      "recipe_decoder.layers.8.fc2.bias\n",
      "recipe_decoder.layers.8.layer_norms.0.weight\n",
      "recipe_decoder.layers.8.layer_norms.0.bias\n",
      "recipe_decoder.layers.8.layer_norms.1.weight\n",
      "recipe_decoder.layers.8.layer_norms.1.bias\n",
      "recipe_decoder.layers.8.layer_norms.2.weight\n",
      "recipe_decoder.layers.8.layer_norms.2.bias\n",
      "recipe_decoder.layers.9.self_attn.in_proj_weight\n",
      "recipe_decoder.layers.9.self_attn.in_proj_bias\n",
      "recipe_decoder.layers.9.self_attn.out_proj.weight\n",
      "recipe_decoder.layers.9.self_attn.out_proj.bias\n",
      "recipe_decoder.layers.9.cond_att.in_proj_weight\n",
      "recipe_decoder.layers.9.cond_att.in_proj_bias\n",
      "recipe_decoder.layers.9.cond_att.out_proj.weight\n",
      "recipe_decoder.layers.9.cond_att.out_proj.bias\n",
      "recipe_decoder.layers.9.fc1.weight\n",
      "recipe_decoder.layers.9.fc1.bias\n",
      "recipe_decoder.layers.9.fc2.weight\n",
      "recipe_decoder.layers.9.fc2.bias\n",
      "recipe_decoder.layers.9.layer_norms.0.weight\n",
      "recipe_decoder.layers.9.layer_norms.0.bias\n",
      "recipe_decoder.layers.9.layer_norms.1.weight\n",
      "recipe_decoder.layers.9.layer_norms.1.bias\n",
      "recipe_decoder.layers.9.layer_norms.2.weight\n",
      "recipe_decoder.layers.9.layer_norms.2.bias\n",
      "recipe_decoder.layers.10.self_attn.in_proj_weight\n",
      "recipe_decoder.layers.10.self_attn.in_proj_bias\n",
      "recipe_decoder.layers.10.self_attn.out_proj.weight\n",
      "recipe_decoder.layers.10.self_attn.out_proj.bias\n",
      "recipe_decoder.layers.10.cond_att.in_proj_weight\n",
      "recipe_decoder.layers.10.cond_att.in_proj_bias\n",
      "recipe_decoder.layers.10.cond_att.out_proj.weight\n",
      "recipe_decoder.layers.10.cond_att.out_proj.bias\n",
      "recipe_decoder.layers.10.fc1.weight\n",
      "recipe_decoder.layers.10.fc1.bias\n",
      "recipe_decoder.layers.10.fc2.weight\n",
      "recipe_decoder.layers.10.fc2.bias\n",
      "recipe_decoder.layers.10.layer_norms.0.weight\n",
      "recipe_decoder.layers.10.layer_norms.0.bias\n",
      "recipe_decoder.layers.10.layer_norms.1.weight\n",
      "recipe_decoder.layers.10.layer_norms.1.bias\n",
      "recipe_decoder.layers.10.layer_norms.2.weight\n",
      "recipe_decoder.layers.10.layer_norms.2.bias\n",
      "recipe_decoder.layers.11.self_attn.in_proj_weight\n",
      "recipe_decoder.layers.11.self_attn.in_proj_bias\n",
      "recipe_decoder.layers.11.self_attn.out_proj.weight\n",
      "recipe_decoder.layers.11.self_attn.out_proj.bias\n",
      "recipe_decoder.layers.11.cond_att.in_proj_weight\n",
      "recipe_decoder.layers.11.cond_att.in_proj_bias\n",
      "recipe_decoder.layers.11.cond_att.out_proj.weight\n",
      "recipe_decoder.layers.11.cond_att.out_proj.bias\n",
      "recipe_decoder.layers.11.fc1.weight\n",
      "recipe_decoder.layers.11.fc1.bias\n",
      "recipe_decoder.layers.11.fc2.weight\n",
      "recipe_decoder.layers.11.fc2.bias\n",
      "recipe_decoder.layers.11.layer_norms.0.weight\n",
      "recipe_decoder.layers.11.layer_norms.0.bias\n",
      "recipe_decoder.layers.11.layer_norms.1.weight\n",
      "recipe_decoder.layers.11.layer_norms.1.bias\n",
      "recipe_decoder.layers.11.layer_norms.2.weight\n",
      "recipe_decoder.layers.11.layer_norms.2.bias\n",
      "recipe_decoder.layers.12.self_attn.in_proj_weight\n",
      "recipe_decoder.layers.12.self_attn.in_proj_bias\n",
      "recipe_decoder.layers.12.self_attn.out_proj.weight\n",
      "recipe_decoder.layers.12.self_attn.out_proj.bias\n",
      "recipe_decoder.layers.12.cond_att.in_proj_weight\n",
      "recipe_decoder.layers.12.cond_att.in_proj_bias\n",
      "recipe_decoder.layers.12.cond_att.out_proj.weight\n",
      "recipe_decoder.layers.12.cond_att.out_proj.bias\n",
      "recipe_decoder.layers.12.fc1.weight\n",
      "recipe_decoder.layers.12.fc1.bias\n",
      "recipe_decoder.layers.12.fc2.weight\n",
      "recipe_decoder.layers.12.fc2.bias\n",
      "recipe_decoder.layers.12.layer_norms.0.weight\n",
      "recipe_decoder.layers.12.layer_norms.0.bias\n",
      "recipe_decoder.layers.12.layer_norms.1.weight\n",
      "recipe_decoder.layers.12.layer_norms.1.bias\n",
      "recipe_decoder.layers.12.layer_norms.2.weight\n",
      "recipe_decoder.layers.12.layer_norms.2.bias\n",
      "recipe_decoder.layers.13.self_attn.in_proj_weight\n",
      "recipe_decoder.layers.13.self_attn.in_proj_bias\n",
      "recipe_decoder.layers.13.self_attn.out_proj.weight\n",
      "recipe_decoder.layers.13.self_attn.out_proj.bias\n",
      "recipe_decoder.layers.13.cond_att.in_proj_weight\n",
      "recipe_decoder.layers.13.cond_att.in_proj_bias\n",
      "recipe_decoder.layers.13.cond_att.out_proj.weight\n",
      "recipe_decoder.layers.13.cond_att.out_proj.bias\n",
      "recipe_decoder.layers.13.fc1.weight\n",
      "recipe_decoder.layers.13.fc1.bias\n",
      "recipe_decoder.layers.13.fc2.weight\n",
      "recipe_decoder.layers.13.fc2.bias\n",
      "recipe_decoder.layers.13.layer_norms.0.weight\n",
      "recipe_decoder.layers.13.layer_norms.0.bias\n",
      "recipe_decoder.layers.13.layer_norms.1.weight\n",
      "recipe_decoder.layers.13.layer_norms.1.bias\n",
      "recipe_decoder.layers.13.layer_norms.2.weight\n",
      "recipe_decoder.layers.13.layer_norms.2.bias\n",
      "recipe_decoder.layers.14.self_attn.in_proj_weight\n",
      "recipe_decoder.layers.14.self_attn.in_proj_bias\n",
      "recipe_decoder.layers.14.self_attn.out_proj.weight\n",
      "recipe_decoder.layers.14.self_attn.out_proj.bias\n",
      "recipe_decoder.layers.14.cond_att.in_proj_weight\n",
      "recipe_decoder.layers.14.cond_att.in_proj_bias\n",
      "recipe_decoder.layers.14.cond_att.out_proj.weight\n",
      "recipe_decoder.layers.14.cond_att.out_proj.bias\n",
      "recipe_decoder.layers.14.fc1.weight\n",
      "recipe_decoder.layers.14.fc1.bias\n",
      "recipe_decoder.layers.14.fc2.weight\n",
      "recipe_decoder.layers.14.fc2.bias\n",
      "recipe_decoder.layers.14.layer_norms.0.weight\n",
      "recipe_decoder.layers.14.layer_norms.0.bias\n",
      "recipe_decoder.layers.14.layer_norms.1.weight\n",
      "recipe_decoder.layers.14.layer_norms.1.bias\n",
      "recipe_decoder.layers.14.layer_norms.2.weight\n",
      "recipe_decoder.layers.14.layer_norms.2.bias\n",
      "recipe_decoder.layers.15.self_attn.in_proj_weight\n",
      "recipe_decoder.layers.15.self_attn.in_proj_bias\n",
      "recipe_decoder.layers.15.self_attn.out_proj.weight\n",
      "recipe_decoder.layers.15.self_attn.out_proj.bias\n",
      "recipe_decoder.layers.15.cond_att.in_proj_weight\n",
      "recipe_decoder.layers.15.cond_att.in_proj_bias\n",
      "recipe_decoder.layers.15.cond_att.out_proj.weight\n",
      "recipe_decoder.layers.15.cond_att.out_proj.bias\n",
      "recipe_decoder.layers.15.fc1.weight\n",
      "recipe_decoder.layers.15.fc1.bias\n",
      "recipe_decoder.layers.15.fc2.weight\n",
      "recipe_decoder.layers.15.fc2.bias\n",
      "recipe_decoder.layers.15.layer_norms.0.weight\n",
      "recipe_decoder.layers.15.layer_norms.0.bias\n",
      "recipe_decoder.layers.15.layer_norms.1.weight\n",
      "recipe_decoder.layers.15.layer_norms.1.bias\n",
      "recipe_decoder.layers.15.layer_norms.2.weight\n",
      "recipe_decoder.layers.15.layer_norms.2.bias\n",
      "recipe_decoder.linear.weight\n",
      "recipe_decoder.linear.bias\n",
      "image_encoder.vit.embeddings.cls_token\n",
      "image_encoder.vit.embeddings.position_embeddings\n",
      "image_encoder.vit.embeddings.patch_embeddings.projection.weight\n",
      "image_encoder.vit.embeddings.patch_embeddings.projection.bias\n",
      "image_encoder.vit.encoder.layer.0.attention.attention.query.weight\n",
      "image_encoder.vit.encoder.layer.0.attention.attention.query.bias\n",
      "image_encoder.vit.encoder.layer.0.attention.attention.key.weight\n",
      "image_encoder.vit.encoder.layer.0.attention.attention.key.bias\n",
      "image_encoder.vit.encoder.layer.0.attention.attention.value.weight\n",
      "image_encoder.vit.encoder.layer.0.attention.attention.value.bias\n",
      "image_encoder.vit.encoder.layer.0.attention.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.0.attention.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.0.intermediate.dense.weight\n",
      "image_encoder.vit.encoder.layer.0.intermediate.dense.bias\n",
      "image_encoder.vit.encoder.layer.0.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.0.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.0.layernorm_before.weight\n",
      "image_encoder.vit.encoder.layer.0.layernorm_before.bias\n",
      "image_encoder.vit.encoder.layer.0.layernorm_after.weight\n",
      "image_encoder.vit.encoder.layer.0.layernorm_after.bias\n",
      "image_encoder.vit.encoder.layer.1.attention.attention.query.weight\n",
      "image_encoder.vit.encoder.layer.1.attention.attention.query.bias\n",
      "image_encoder.vit.encoder.layer.1.attention.attention.key.weight\n",
      "image_encoder.vit.encoder.layer.1.attention.attention.key.bias\n",
      "image_encoder.vit.encoder.layer.1.attention.attention.value.weight\n",
      "image_encoder.vit.encoder.layer.1.attention.attention.value.bias\n",
      "image_encoder.vit.encoder.layer.1.attention.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.1.attention.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.1.intermediate.dense.weight\n",
      "image_encoder.vit.encoder.layer.1.intermediate.dense.bias\n",
      "image_encoder.vit.encoder.layer.1.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.1.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.1.layernorm_before.weight\n",
      "image_encoder.vit.encoder.layer.1.layernorm_before.bias\n",
      "image_encoder.vit.encoder.layer.1.layernorm_after.weight\n",
      "image_encoder.vit.encoder.layer.1.layernorm_after.bias\n",
      "image_encoder.vit.encoder.layer.2.attention.attention.query.weight\n",
      "image_encoder.vit.encoder.layer.2.attention.attention.query.bias\n",
      "image_encoder.vit.encoder.layer.2.attention.attention.key.weight\n",
      "image_encoder.vit.encoder.layer.2.attention.attention.key.bias\n",
      "image_encoder.vit.encoder.layer.2.attention.attention.value.weight\n",
      "image_encoder.vit.encoder.layer.2.attention.attention.value.bias\n",
      "image_encoder.vit.encoder.layer.2.attention.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.2.attention.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.2.intermediate.dense.weight\n",
      "image_encoder.vit.encoder.layer.2.intermediate.dense.bias\n",
      "image_encoder.vit.encoder.layer.2.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.2.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.2.layernorm_before.weight\n",
      "image_encoder.vit.encoder.layer.2.layernorm_before.bias\n",
      "image_encoder.vit.encoder.layer.2.layernorm_after.weight\n",
      "image_encoder.vit.encoder.layer.2.layernorm_after.bias\n",
      "image_encoder.vit.encoder.layer.3.attention.attention.query.weight\n",
      "image_encoder.vit.encoder.layer.3.attention.attention.query.bias\n",
      "image_encoder.vit.encoder.layer.3.attention.attention.key.weight\n",
      "image_encoder.vit.encoder.layer.3.attention.attention.key.bias\n",
      "image_encoder.vit.encoder.layer.3.attention.attention.value.weight\n",
      "image_encoder.vit.encoder.layer.3.attention.attention.value.bias\n",
      "image_encoder.vit.encoder.layer.3.attention.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.3.attention.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.3.intermediate.dense.weight\n",
      "image_encoder.vit.encoder.layer.3.intermediate.dense.bias\n",
      "image_encoder.vit.encoder.layer.3.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.3.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.3.layernorm_before.weight\n",
      "image_encoder.vit.encoder.layer.3.layernorm_before.bias\n",
      "image_encoder.vit.encoder.layer.3.layernorm_after.weight\n",
      "image_encoder.vit.encoder.layer.3.layernorm_after.bias\n",
      "image_encoder.vit.encoder.layer.4.attention.attention.query.weight\n",
      "image_encoder.vit.encoder.layer.4.attention.attention.query.bias\n",
      "image_encoder.vit.encoder.layer.4.attention.attention.key.weight\n",
      "image_encoder.vit.encoder.layer.4.attention.attention.key.bias\n",
      "image_encoder.vit.encoder.layer.4.attention.attention.value.weight\n",
      "image_encoder.vit.encoder.layer.4.attention.attention.value.bias\n",
      "image_encoder.vit.encoder.layer.4.attention.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.4.attention.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.4.intermediate.dense.weight\n",
      "image_encoder.vit.encoder.layer.4.intermediate.dense.bias\n",
      "image_encoder.vit.encoder.layer.4.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.4.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.4.layernorm_before.weight\n",
      "image_encoder.vit.encoder.layer.4.layernorm_before.bias\n",
      "image_encoder.vit.encoder.layer.4.layernorm_after.weight\n",
      "image_encoder.vit.encoder.layer.4.layernorm_after.bias\n",
      "image_encoder.vit.encoder.layer.5.attention.attention.query.weight\n",
      "image_encoder.vit.encoder.layer.5.attention.attention.query.bias\n",
      "image_encoder.vit.encoder.layer.5.attention.attention.key.weight\n",
      "image_encoder.vit.encoder.layer.5.attention.attention.key.bias\n",
      "image_encoder.vit.encoder.layer.5.attention.attention.value.weight\n",
      "image_encoder.vit.encoder.layer.5.attention.attention.value.bias\n",
      "image_encoder.vit.encoder.layer.5.attention.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.5.attention.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.5.intermediate.dense.weight\n",
      "image_encoder.vit.encoder.layer.5.intermediate.dense.bias\n",
      "image_encoder.vit.encoder.layer.5.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.5.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.5.layernorm_before.weight\n",
      "image_encoder.vit.encoder.layer.5.layernorm_before.bias\n",
      "image_encoder.vit.encoder.layer.5.layernorm_after.weight\n",
      "image_encoder.vit.encoder.layer.5.layernorm_after.bias\n",
      "image_encoder.vit.encoder.layer.6.attention.attention.query.weight\n",
      "image_encoder.vit.encoder.layer.6.attention.attention.query.bias\n",
      "image_encoder.vit.encoder.layer.6.attention.attention.key.weight\n",
      "image_encoder.vit.encoder.layer.6.attention.attention.key.bias\n",
      "image_encoder.vit.encoder.layer.6.attention.attention.value.weight\n",
      "image_encoder.vit.encoder.layer.6.attention.attention.value.bias\n",
      "image_encoder.vit.encoder.layer.6.attention.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.6.attention.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.6.intermediate.dense.weight\n",
      "image_encoder.vit.encoder.layer.6.intermediate.dense.bias\n",
      "image_encoder.vit.encoder.layer.6.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.6.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.6.layernorm_before.weight\n",
      "image_encoder.vit.encoder.layer.6.layernorm_before.bias\n",
      "image_encoder.vit.encoder.layer.6.layernorm_after.weight\n",
      "image_encoder.vit.encoder.layer.6.layernorm_after.bias\n",
      "image_encoder.vit.encoder.layer.7.attention.attention.query.weight\n",
      "image_encoder.vit.encoder.layer.7.attention.attention.query.bias\n",
      "image_encoder.vit.encoder.layer.7.attention.attention.key.weight\n",
      "image_encoder.vit.encoder.layer.7.attention.attention.key.bias\n",
      "image_encoder.vit.encoder.layer.7.attention.attention.value.weight\n",
      "image_encoder.vit.encoder.layer.7.attention.attention.value.bias\n",
      "image_encoder.vit.encoder.layer.7.attention.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.7.attention.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.7.intermediate.dense.weight\n",
      "image_encoder.vit.encoder.layer.7.intermediate.dense.bias\n",
      "image_encoder.vit.encoder.layer.7.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.7.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.7.layernorm_before.weight\n",
      "image_encoder.vit.encoder.layer.7.layernorm_before.bias\n",
      "image_encoder.vit.encoder.layer.7.layernorm_after.weight\n",
      "image_encoder.vit.encoder.layer.7.layernorm_after.bias\n",
      "image_encoder.vit.encoder.layer.8.attention.attention.query.weight\n",
      "image_encoder.vit.encoder.layer.8.attention.attention.query.bias\n",
      "image_encoder.vit.encoder.layer.8.attention.attention.key.weight\n",
      "image_encoder.vit.encoder.layer.8.attention.attention.key.bias\n",
      "image_encoder.vit.encoder.layer.8.attention.attention.value.weight\n",
      "image_encoder.vit.encoder.layer.8.attention.attention.value.bias\n",
      "image_encoder.vit.encoder.layer.8.attention.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.8.attention.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.8.intermediate.dense.weight\n",
      "image_encoder.vit.encoder.layer.8.intermediate.dense.bias\n",
      "image_encoder.vit.encoder.layer.8.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.8.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.8.layernorm_before.weight\n",
      "image_encoder.vit.encoder.layer.8.layernorm_before.bias\n",
      "image_encoder.vit.encoder.layer.8.layernorm_after.weight\n",
      "image_encoder.vit.encoder.layer.8.layernorm_after.bias\n",
      "image_encoder.vit.encoder.layer.9.attention.attention.query.weight\n",
      "image_encoder.vit.encoder.layer.9.attention.attention.query.bias\n",
      "image_encoder.vit.encoder.layer.9.attention.attention.key.weight\n",
      "image_encoder.vit.encoder.layer.9.attention.attention.key.bias\n",
      "image_encoder.vit.encoder.layer.9.attention.attention.value.weight\n",
      "image_encoder.vit.encoder.layer.9.attention.attention.value.bias\n",
      "image_encoder.vit.encoder.layer.9.attention.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.9.attention.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.9.intermediate.dense.weight\n",
      "image_encoder.vit.encoder.layer.9.intermediate.dense.bias\n",
      "image_encoder.vit.encoder.layer.9.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.9.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.9.layernorm_before.weight\n",
      "image_encoder.vit.encoder.layer.9.layernorm_before.bias\n",
      "image_encoder.vit.encoder.layer.9.layernorm_after.weight\n",
      "image_encoder.vit.encoder.layer.9.layernorm_after.bias\n",
      "image_encoder.vit.encoder.layer.10.attention.attention.query.weight\n",
      "image_encoder.vit.encoder.layer.10.attention.attention.query.bias\n",
      "image_encoder.vit.encoder.layer.10.attention.attention.key.weight\n",
      "image_encoder.vit.encoder.layer.10.attention.attention.key.bias\n",
      "image_encoder.vit.encoder.layer.10.attention.attention.value.weight\n",
      "image_encoder.vit.encoder.layer.10.attention.attention.value.bias\n",
      "image_encoder.vit.encoder.layer.10.attention.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.10.attention.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.10.intermediate.dense.weight\n",
      "image_encoder.vit.encoder.layer.10.intermediate.dense.bias\n",
      "image_encoder.vit.encoder.layer.10.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.10.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.10.layernorm_before.weight\n",
      "image_encoder.vit.encoder.layer.10.layernorm_before.bias\n",
      "image_encoder.vit.encoder.layer.10.layernorm_after.weight\n",
      "image_encoder.vit.encoder.layer.10.layernorm_after.bias\n",
      "image_encoder.vit.encoder.layer.11.attention.attention.query.weight\n",
      "image_encoder.vit.encoder.layer.11.attention.attention.query.bias\n",
      "image_encoder.vit.encoder.layer.11.attention.attention.key.weight\n",
      "image_encoder.vit.encoder.layer.11.attention.attention.key.bias\n",
      "image_encoder.vit.encoder.layer.11.attention.attention.value.weight\n",
      "image_encoder.vit.encoder.layer.11.attention.attention.value.bias\n",
      "image_encoder.vit.encoder.layer.11.attention.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.11.attention.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.11.intermediate.dense.weight\n",
      "image_encoder.vit.encoder.layer.11.intermediate.dense.bias\n",
      "image_encoder.vit.encoder.layer.11.output.dense.weight\n",
      "image_encoder.vit.encoder.layer.11.output.dense.bias\n",
      "image_encoder.vit.encoder.layer.11.layernorm_before.weight\n",
      "image_encoder.vit.encoder.layer.11.layernorm_before.bias\n",
      "image_encoder.vit.encoder.layer.11.layernorm_after.weight\n",
      "image_encoder.vit.encoder.layer.11.layernorm_after.bias\n",
      "image_encoder.vit.layernorm.weight\n",
      "image_encoder.vit.layernorm.bias\n",
      "image_encoder.vit.pooler.dense.weight\n",
      "image_encoder.vit.pooler.dense.bias\n",
      "image_encoder.linear.weight\n",
      "image_encoder.linear.bias\n",
      "image_encoder.sequential.0.weight\n",
      "image_encoder.sequential.0.bias\n",
      "ingredient_decoder.embed_tokens.weight\n",
      "ingredient_decoder.layer_norms_in.0.weight\n",
      "ingredient_decoder.layer_norms_in.0.bias\n",
      "ingredient_decoder.layer_norms_in.1.weight\n",
      "ingredient_decoder.layer_norms_in.1.bias\n",
      "ingredient_decoder.layer_norms_in.2.weight\n",
      "ingredient_decoder.layer_norms_in.2.bias\n",
      "ingredient_decoder.layers.0.self_attn.in_proj_weight\n",
      "ingredient_decoder.layers.0.self_attn.in_proj_bias\n",
      "ingredient_decoder.layers.0.self_attn.out_proj.weight\n",
      "ingredient_decoder.layers.0.self_attn.out_proj.bias\n",
      "ingredient_decoder.layers.0.cond_att.in_proj_weight\n",
      "ingredient_decoder.layers.0.cond_att.in_proj_bias\n",
      "ingredient_decoder.layers.0.cond_att.out_proj.weight\n",
      "ingredient_decoder.layers.0.cond_att.out_proj.bias\n",
      "ingredient_decoder.layers.0.fc1.weight\n",
      "ingredient_decoder.layers.0.fc1.bias\n",
      "ingredient_decoder.layers.0.fc2.weight\n",
      "ingredient_decoder.layers.0.fc2.bias\n",
      "ingredient_decoder.layers.0.layer_norms.0.weight\n",
      "ingredient_decoder.layers.0.layer_norms.0.bias\n",
      "ingredient_decoder.layers.0.layer_norms.1.weight\n",
      "ingredient_decoder.layers.0.layer_norms.1.bias\n",
      "ingredient_decoder.layers.0.layer_norms.2.weight\n",
      "ingredient_decoder.layers.0.layer_norms.2.bias\n",
      "ingredient_decoder.layers.0.last_ln.weight\n",
      "ingredient_decoder.layers.0.last_ln.bias\n",
      "ingredient_decoder.layers.1.self_attn.in_proj_weight\n",
      "ingredient_decoder.layers.1.self_attn.in_proj_bias\n",
      "ingredient_decoder.layers.1.self_attn.out_proj.weight\n",
      "ingredient_decoder.layers.1.self_attn.out_proj.bias\n",
      "ingredient_decoder.layers.1.cond_att.in_proj_weight\n",
      "ingredient_decoder.layers.1.cond_att.in_proj_bias\n",
      "ingredient_decoder.layers.1.cond_att.out_proj.weight\n",
      "ingredient_decoder.layers.1.cond_att.out_proj.bias\n",
      "ingredient_decoder.layers.1.fc1.weight\n",
      "ingredient_decoder.layers.1.fc1.bias\n",
      "ingredient_decoder.layers.1.fc2.weight\n",
      "ingredient_decoder.layers.1.fc2.bias\n",
      "ingredient_decoder.layers.1.layer_norms.0.weight\n",
      "ingredient_decoder.layers.1.layer_norms.0.bias\n",
      "ingredient_decoder.layers.1.layer_norms.1.weight\n",
      "ingredient_decoder.layers.1.layer_norms.1.bias\n",
      "ingredient_decoder.layers.1.layer_norms.2.weight\n",
      "ingredient_decoder.layers.1.layer_norms.2.bias\n",
      "ingredient_decoder.layers.1.last_ln.weight\n",
      "ingredient_decoder.layers.1.last_ln.bias\n",
      "ingredient_decoder.layers.2.self_attn.in_proj_weight\n",
      "ingredient_decoder.layers.2.self_attn.in_proj_bias\n",
      "ingredient_decoder.layers.2.self_attn.out_proj.weight\n",
      "ingredient_decoder.layers.2.self_attn.out_proj.bias\n",
      "ingredient_decoder.layers.2.cond_att.in_proj_weight\n",
      "ingredient_decoder.layers.2.cond_att.in_proj_bias\n",
      "ingredient_decoder.layers.2.cond_att.out_proj.weight\n",
      "ingredient_decoder.layers.2.cond_att.out_proj.bias\n",
      "ingredient_decoder.layers.2.fc1.weight\n",
      "ingredient_decoder.layers.2.fc1.bias\n",
      "ingredient_decoder.layers.2.fc2.weight\n",
      "ingredient_decoder.layers.2.fc2.bias\n",
      "ingredient_decoder.layers.2.layer_norms.0.weight\n",
      "ingredient_decoder.layers.2.layer_norms.0.bias\n",
      "ingredient_decoder.layers.2.layer_norms.1.weight\n",
      "ingredient_decoder.layers.2.layer_norms.1.bias\n",
      "ingredient_decoder.layers.2.layer_norms.2.weight\n",
      "ingredient_decoder.layers.2.layer_norms.2.bias\n",
      "ingredient_decoder.layers.2.last_ln.weight\n",
      "ingredient_decoder.layers.2.last_ln.bias\n",
      "ingredient_decoder.layers.3.self_attn.in_proj_weight\n",
      "ingredient_decoder.layers.3.self_attn.in_proj_bias\n",
      "ingredient_decoder.layers.3.self_attn.out_proj.weight\n",
      "ingredient_decoder.layers.3.self_attn.out_proj.bias\n",
      "ingredient_decoder.layers.3.cond_att.in_proj_weight\n",
      "ingredient_decoder.layers.3.cond_att.in_proj_bias\n",
      "ingredient_decoder.layers.3.cond_att.out_proj.weight\n",
      "ingredient_decoder.layers.3.cond_att.out_proj.bias\n",
      "ingredient_decoder.layers.3.fc1.weight\n",
      "ingredient_decoder.layers.3.fc1.bias\n",
      "ingredient_decoder.layers.3.fc2.weight\n",
      "ingredient_decoder.layers.3.fc2.bias\n",
      "ingredient_decoder.layers.3.layer_norms.0.weight\n",
      "ingredient_decoder.layers.3.layer_norms.0.bias\n",
      "ingredient_decoder.layers.3.layer_norms.1.weight\n",
      "ingredient_decoder.layers.3.layer_norms.1.bias\n",
      "ingredient_decoder.layers.3.layer_norms.2.weight\n",
      "ingredient_decoder.layers.3.layer_norms.2.bias\n",
      "ingredient_decoder.layers.3.last_ln.weight\n",
      "ingredient_decoder.layers.3.last_ln.bias\n",
      "ingredient_decoder.linear.weight\n",
      "ingredient_decoder.linear.bias\n"
     ]
    }
   ],
   "source": [
    "vit_model_path = '/data/prateek/github/see-food/checkpoints_10k/inversecooking/ingr_only_vit/checkpoints/modelbest.ckpt'\n",
    "partial_vit_state_dict = torch.load(vit_model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "for keys in partial_vit_state_dict.keys():\n",
    "    print(keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EncoderVisionTransformer:\n\tMissing key(s) in state_dict: \"vit.embeddings.cls_token\", \"vit.embeddings.position_embeddings\", \"vit.embeddings.patch_embeddings.projection.weight\", \"vit.embeddings.patch_embeddings.projection.bias\", \"vit.encoder.layer.0.attention.attention.query.weight\", \"vit.encoder.layer.0.attention.attention.query.bias\", \"vit.encoder.layer.0.attention.attention.key.weight\", \"vit.encoder.layer.0.attention.attention.key.bias\", \"vit.encoder.layer.0.attention.attention.value.weight\", \"vit.encoder.layer.0.attention.attention.value.bias\", \"vit.encoder.layer.0.attention.output.dense.weight\", \"vit.encoder.layer.0.attention.output.dense.bias\", \"vit.encoder.layer.0.intermediate.dense.weight\", \"vit.encoder.layer.0.intermediate.dense.bias\", \"vit.encoder.layer.0.output.dense.weight\", \"vit.encoder.layer.0.output.dense.bias\", \"vit.encoder.layer.0.layernorm_before.weight\", \"vit.encoder.layer.0.layernorm_before.bias\", \"vit.encoder.layer.0.layernorm_after.weight\", \"vit.encoder.layer.0.layernorm_after.bias\", \"vit.encoder.layer.1.attention.attention.query.weight\", \"vit.encoder.layer.1.attention.attention.query.bias\", \"vit.encoder.layer.1.attention.attention.key.weight\", \"vit.encoder.layer.1.attention.attention.key.bias\", \"vit.encoder.layer.1.attention.attention.value.weight\", \"vit.encoder.layer.1.attention.attention.value.bias\", \"vit.encoder.layer.1.attention.output.dense.weight\", \"vit.encoder.layer.1.attention.output.dense.bias\", \"vit.encoder.layer.1.intermediate.dense.weight\", \"vit.encoder.layer.1.intermediate.dense.bias\", \"vit.encoder.layer.1.output.dense.weight\", \"vit.encoder.layer.1.output.dense.bias\", \"vit.encoder.layer.1.layernorm_before.weight\", \"vit.encoder.layer.1.layernorm_before.bias\", \"vit.encoder.layer.1.layernorm_after.weight\", \"vit.encoder.layer.1.layernorm_after.bias\", \"vit.encoder.layer.2.attention.attention.query.weight\", \"vit.encoder.layer.2.attention.attention.query.bias\", \"vit.encoder.layer.2.attention.attention.key.weight\", \"vit.encoder.layer.2.attention.attention.key.bias\", \"vit.encoder.layer.2.attention.attention.value.weight\", \"vit.encoder.layer.2.attention.attention.value.bias\", \"vit.encoder.layer.2.attention.output.dense.weight\", \"vit.encoder.layer.2.attention.output.dense.bias\", \"vit.encoder.layer.2.intermediate.dense.weight\", \"vit.encoder.layer.2.intermediate.dense.bias\", \"vit.encoder.layer.2.output.dense.weight\", \"vit.encoder.layer.2.output.dense.bias\", \"vit.encoder.layer.2.layernorm_before.weight\", \"vit.encoder.layer.2.layernorm_before.bias\", \"vit.encoder.layer.2.layernorm_after.weight\", \"vit.encoder.layer.2.layernorm_after.bias\", \"vit.encoder.layer.3.attention.attention.query.weight\", \"vit.encoder.layer.3.attention.attention.query.bias\", \"vit.encoder.layer.3.attention.attention.key.weight\", \"vit.encoder.layer.3.attention.attention.key.bias\", \"vit.encoder.layer.3.attention.attention.value.weight\", \"vit.encoder.layer.3.attention.attention.value.bias\", \"vit.encoder.layer.3.attention.output.dense.weight\", \"vit.encoder.layer.3.attention.output.dense.bias\", \"vit.encoder.layer.3.intermediate.dense.weight\", \"vit.encoder.layer.3.intermediate.dense.bias\", \"vit.encoder.layer.3.output.dense.weight\", \"vit.encoder.layer.3.output.dense.bias\", \"vit.encoder.layer.3.layernorm_before.weight\", \"vit.encoder.layer.3.layernorm_before.bias\", \"vit.encoder.layer.3.layernorm_after.weight\", \"vit.encoder.layer.3.layernorm_after.bias\", \"vit.encoder.layer.4.attention.attention.query.weight\", \"vit.encoder.layer.4.attention.attention.query.bias\", \"vit.encoder.layer.4.attention.attention.key.weight\", \"vit.encoder.layer.4.attention.attention.key.bias\", \"vit.encoder.layer.4.attention.attention.value.weight\", \"vit.encoder.layer.4.attention.attention.value.bias\", \"vit.encoder.layer.4.attention.output.dense.weight\", \"vit.encoder.layer.4.attention.output.dense.bias\", \"vit.encoder.layer.4.intermediate.dense.weight\", \"vit.encoder.layer.4.intermediate.dense.bias\", \"vit.encoder.layer.4.output.dense.weight\", \"vit.encoder.layer.4.output.dense.bias\", \"vit.encoder.layer.4.layernorm_before.weight\", \"vit.encoder.layer.4.layernorm_before.bias\", \"vit.encoder.layer.4.layernorm_after.weight\", \"vit.encoder.layer.4.layernorm_after.bias\", \"vit.encoder.layer.5.attention.attention.query.weight\", \"vit.encoder.layer.5.attention.attention.query.bias\", \"vit.encoder.layer.5.attention.attention.key.weight\", \"vit.encoder.layer.5.attention.attention.key.bias\", \"vit.encoder.layer.5.attention.attention.value.weight\", \"vit.encoder.layer.5.attention.attention.value.bias\", \"vit.encoder.layer.5.attention.output.dense.weight\", \"vit.encoder.layer.5.attention.output.dense.bias\", \"vit.encoder.layer.5.intermediate.dense.weight\", \"vit.encoder.layer.5.intermediate.dense.bias\", \"vit.encoder.layer.5.output.dense.weight\", \"vit.encoder.layer.5.output.dense.bias\", \"vit.encoder.layer.5.layernorm_before.weight\", \"vit.encoder.layer.5.layernorm_before.bias\", \"vit.encoder.layer.5.layernorm_after.weight\", \"vit.encoder.layer.5.layernorm_after.bias\", \"vit.encoder.layer.6.attention.attention.query.weight\", \"vit.encoder.layer.6.attention.attention.query.bias\", \"vit.encoder.layer.6.attention.attention.key.weight\", \"vit.encoder.layer.6.attention.attention.key.bias\", \"vit.encoder.layer.6.attention.attention.value.weight\", \"vit.encoder.layer.6.attention.attention.value.bias\", \"vit.encoder.layer.6.attention.output.dense.weight\", \"vit.encoder.layer.6.attention.output.dense.bias\", \"vit.encoder.layer.6.intermediate.dense.weight\", \"vit.encoder.layer.6.intermediate.dense.bias\", \"vit.encoder.layer.6.output.dense.weight\", \"vit.encoder.layer.6.output.dense.bias\", \"vit.encoder.layer.6.layernorm_before.weight\", \"vit.encoder.layer.6.layernorm_before.bias\", \"vit.encoder.layer.6.layernorm_after.weight\", \"vit.encoder.layer.6.layernorm_after.bias\", \"vit.encoder.layer.7.attention.attention.query.weight\", \"vit.encoder.layer.7.attention.attention.query.bias\", \"vit.encoder.layer.7.attention.attention.key.weight\", \"vit.encoder.layer.7.attention.attention.key.bias\", \"vit.encoder.layer.7.attention.attention.value.weight\", \"vit.encoder.layer.7.attention.attention.value.bias\", \"vit.encoder.layer.7.attention.output.dense.weight\", \"vit.encoder.layer.7.attention.output.dense.bias\", \"vit.encoder.layer.7.intermediate.dense.weight\", \"vit.encoder.layer.7.intermediate.dense.bias\", \"vit.encoder.layer.7.output.dense.weight\", \"vit.encoder.layer.7.output.dense.bias\", \"vit.encoder.layer.7.layernorm_before.weight\", \"vit.encoder.layer.7.layernorm_before.bias\", \"vit.encoder.layer.7.layernorm_after.weight\", \"vit.encoder.layer.7.layernorm_after.bias\", \"vit.encoder.layer.8.attention.attention.query.weight\", \"vit.encoder.layer.8.attention.attention.query.bias\", \"vit.encoder.layer.8.attention.attention.key.weight\", \"vit.encoder.layer.8.attention.attention.key.bias\", \"vit.encoder.layer.8.attention.attention.value.weight\", \"vit.encoder.layer.8.attention.attention.value.bias\", \"vit.encoder.layer.8.attention.output.dense.weight\", \"vit.encoder.layer.8.attention.output.dense.bias\", \"vit.encoder.layer.8.intermediate.dense.weight\", \"vit.encoder.layer.8.intermediate.dense.bias\", \"vit.encoder.layer.8.output.dense.weight\", \"vit.encoder.layer.8.output.dense.bias\", \"vit.encoder.layer.8.layernorm_before.weight\", \"vit.encoder.layer.8.layernorm_before.bias\", \"vit.encoder.layer.8.layernorm_after.weight\", \"vit.encoder.layer.8.layernorm_after.bias\", \"vit.encoder.layer.9.attention.attention.query.weight\", \"vit.encoder.layer.9.attention.attention.query.bias\", \"vit.encoder.layer.9.attention.attention.key.weight\", \"vit.encoder.layer.9.attention.attention.key.bias\", \"vit.encoder.layer.9.attention.attention.value.weight\", \"vit.encoder.layer.9.attention.attention.value.bias\", \"vit.encoder.layer.9.attention.output.dense.weight\", \"vit.encoder.layer.9.attention.output.dense.bias\", \"vit.encoder.layer.9.intermediate.dense.weight\", \"vit.encoder.layer.9.intermediate.dense.bias\", \"vit.encoder.layer.9.output.dense.weight\", \"vit.encoder.layer.9.output.dense.bias\", \"vit.encoder.layer.9.layernorm_before.weight\", \"vit.encoder.layer.9.layernorm_before.bias\", \"vit.encoder.layer.9.layernorm_after.weight\", \"vit.encoder.layer.9.layernorm_after.bias\", \"vit.encoder.layer.10.attention.attention.query.weight\", \"vit.encoder.layer.10.attention.attention.query.bias\", \"vit.encoder.layer.10.attention.attention.key.weight\", \"vit.encoder.layer.10.attention.attention.key.bias\", \"vit.encoder.layer.10.attention.attention.value.weight\", \"vit.encoder.layer.10.attention.attention.value.bias\", \"vit.encoder.layer.10.attention.output.dense.weight\", \"vit.encoder.layer.10.attention.output.dense.bias\", \"vit.encoder.layer.10.intermediate.dense.weight\", \"vit.encoder.layer.10.intermediate.dense.bias\", \"vit.encoder.layer.10.output.dense.weight\", \"vit.encoder.layer.10.output.dense.bias\", \"vit.encoder.layer.10.layernorm_before.weight\", \"vit.encoder.layer.10.layernorm_before.bias\", \"vit.encoder.layer.10.layernorm_after.weight\", \"vit.encoder.layer.10.layernorm_after.bias\", \"vit.encoder.layer.11.attention.attention.query.weight\", \"vit.encoder.layer.11.attention.attention.query.bias\", \"vit.encoder.layer.11.attention.attention.key.weight\", \"vit.encoder.layer.11.attention.attention.key.bias\", \"vit.encoder.layer.11.attention.attention.value.weight\", \"vit.encoder.layer.11.attention.attention.value.bias\", \"vit.encoder.layer.11.attention.output.dense.weight\", \"vit.encoder.layer.11.attention.output.dense.bias\", \"vit.encoder.layer.11.intermediate.dense.weight\", \"vit.encoder.layer.11.intermediate.dense.bias\", \"vit.encoder.layer.11.output.dense.weight\", \"vit.encoder.layer.11.output.dense.bias\", \"vit.encoder.layer.11.layernorm_before.weight\", \"vit.encoder.layer.11.layernorm_before.bias\", \"vit.encoder.layer.11.layernorm_after.weight\", \"vit.encoder.layer.11.layernorm_after.bias\", \"vit.layernorm.weight\", \"vit.layernorm.bias\", \"vit.pooler.dense.weight\", \"vit.pooler.dense.bias\", \"linear.weight\", \"linear.bias\", \"sequential.0.weight\", \"sequential.0.bias\". \n\tUnexpected key(s) in state_dict: \"ingredient_encoder.linear.weight\", \"recipe_decoder.embed_tokens.weight\", \"recipe_decoder.embed_positions.weight\", \"recipe_decoder.layers.0.self_attn.in_proj_weight\", \"recipe_decoder.layers.0.self_attn.in_proj_bias\", \"recipe_decoder.layers.0.self_attn.out_proj.weight\", \"recipe_decoder.layers.0.self_attn.out_proj.bias\", \"recipe_decoder.layers.0.cond_att.in_proj_weight\", \"recipe_decoder.layers.0.cond_att.in_proj_bias\", \"recipe_decoder.layers.0.cond_att.out_proj.weight\", \"recipe_decoder.layers.0.cond_att.out_proj.bias\", \"recipe_decoder.layers.0.fc1.weight\", \"recipe_decoder.layers.0.fc1.bias\", \"recipe_decoder.layers.0.fc2.weight\", \"recipe_decoder.layers.0.fc2.bias\", \"recipe_decoder.layers.0.layer_norms.0.weight\", \"recipe_decoder.layers.0.layer_norms.0.bias\", \"recipe_decoder.layers.0.layer_norms.1.weight\", \"recipe_decoder.layers.0.layer_norms.1.bias\", \"recipe_decoder.layers.0.layer_norms.2.weight\", \"recipe_decoder.layers.0.layer_norms.2.bias\", \"recipe_decoder.layers.1.self_attn.in_proj_weight\", \"recipe_decoder.layers.1.self_attn.in_proj_bias\", \"recipe_decoder.layers.1.self_attn.out_proj.weight\", \"recipe_decoder.layers.1.self_attn.out_proj.bias\", \"recipe_decoder.layers.1.cond_att.in_proj_weight\", \"recipe_decoder.layers.1.cond_att.in_proj_bias\", \"recipe_decoder.layers.1.cond_att.out_proj.weight\", \"recipe_decoder.layers.1.cond_att.out_proj.bias\", \"recipe_decoder.layers.1.fc1.weight\", \"recipe_decoder.layers.1.fc1.bias\", \"recipe_decoder.layers.1.fc2.weight\", \"recipe_decoder.layers.1.fc2.bias\", \"recipe_decoder.layers.1.layer_norms.0.weight\", \"recipe_decoder.layers.1.layer_norms.0.bias\", \"recipe_decoder.layers.1.layer_norms.1.weight\", \"recipe_decoder.layers.1.layer_norms.1.bias\", \"recipe_decoder.layers.1.layer_norms.2.weight\", \"recipe_decoder.layers.1.layer_norms.2.bias\", \"recipe_decoder.layers.2.self_attn.in_proj_weight\", \"recipe_decoder.layers.2.self_attn.in_proj_bias\", \"recipe_decoder.layers.2.self_attn.out_proj.weight\", \"recipe_decoder.layers.2.self_attn.out_proj.bias\", \"recipe_decoder.layers.2.cond_att.in_proj_weight\", \"recipe_decoder.layers.2.cond_att.in_proj_bias\", \"recipe_decoder.layers.2.cond_att.out_proj.weight\", \"recipe_decoder.layers.2.cond_att.out_proj.bias\", \"recipe_decoder.layers.2.fc1.weight\", \"recipe_decoder.layers.2.fc1.bias\", \"recipe_decoder.layers.2.fc2.weight\", \"recipe_decoder.layers.2.fc2.bias\", \"recipe_decoder.layers.2.layer_norms.0.weight\", \"recipe_decoder.layers.2.layer_norms.0.bias\", \"recipe_decoder.layers.2.layer_norms.1.weight\", \"recipe_decoder.layers.2.layer_norms.1.bias\", \"recipe_decoder.layers.2.layer_norms.2.weight\", \"recipe_decoder.layers.2.layer_norms.2.bias\", \"recipe_decoder.layers.3.self_attn.in_proj_weight\", \"recipe_decoder.layers.3.self_attn.in_proj_bias\", \"recipe_decoder.layers.3.self_attn.out_proj.weight\", \"recipe_decoder.layers.3.self_attn.out_proj.bias\", \"recipe_decoder.layers.3.cond_att.in_proj_weight\", \"recipe_decoder.layers.3.cond_att.in_proj_bias\", \"recipe_decoder.layers.3.cond_att.out_proj.weight\", \"recipe_decoder.layers.3.cond_att.out_proj.bias\", \"recipe_decoder.layers.3.fc1.weight\", \"recipe_decoder.layers.3.fc1.bias\", \"recipe_decoder.layers.3.fc2.weight\", \"recipe_decoder.layers.3.fc2.bias\", \"recipe_decoder.layers.3.layer_norms.0.weight\", \"recipe_decoder.layers.3.layer_norms.0.bias\", \"recipe_decoder.layers.3.layer_norms.1.weight\", \"recipe_decoder.layers.3.layer_norms.1.bias\", \"recipe_decoder.layers.3.layer_norms.2.weight\", \"recipe_decoder.layers.3.layer_norms.2.bias\", \"recipe_decoder.layers.4.self_attn.in_proj_weight\", \"recipe_decoder.layers.4.self_attn.in_proj_bias\", \"recipe_decoder.layers.4.self_attn.out_proj.weight\", \"recipe_decoder.layers.4.self_attn.out_proj.bias\", \"recipe_decoder.layers.4.cond_att.in_proj_weight\", \"recipe_decoder.layers.4.cond_att.in_proj_bias\", \"recipe_decoder.layers.4.cond_att.out_proj.weight\", \"recipe_decoder.layers.4.cond_att.out_proj.bias\", \"recipe_decoder.layers.4.fc1.weight\", \"recipe_decoder.layers.4.fc1.bias\", \"recipe_decoder.layers.4.fc2.weight\", \"recipe_decoder.layers.4.fc2.bias\", \"recipe_decoder.layers.4.layer_norms.0.weight\", \"recipe_decoder.layers.4.layer_norms.0.bias\", \"recipe_decoder.layers.4.layer_norms.1.weight\", \"recipe_decoder.layers.4.layer_norms.1.bias\", \"recipe_decoder.layers.4.layer_norms.2.weight\", \"recipe_decoder.layers.4.layer_norms.2.bias\", \"recipe_decoder.layers.5.self_attn.in_proj_weight\", \"recipe_decoder.layers.5.self_attn.in_proj_bias\", \"recipe_decoder.layers.5.self_attn.out_proj.weight\", \"recipe_decoder.layers.5.self_attn.out_proj.bias\", \"recipe_decoder.layers.5.cond_att.in_proj_weight\", \"recipe_decoder.layers.5.cond_att.in_proj_bias\", \"recipe_decoder.layers.5.cond_att.out_proj.weight\", \"recipe_decoder.layers.5.cond_att.out_proj.bias\", \"recipe_decoder.layers.5.fc1.weight\", \"recipe_decoder.layers.5.fc1.bias\", \"recipe_decoder.layers.5.fc2.weight\", \"recipe_decoder.layers.5.fc2.bias\", \"recipe_decoder.layers.5.layer_norms.0.weight\", \"recipe_decoder.layers.5.layer_norms.0.bias\", \"recipe_decoder.layers.5.layer_norms.1.weight\", \"recipe_decoder.layers.5.layer_norms.1.bias\", \"recipe_decoder.layers.5.layer_norms.2.weight\", \"recipe_decoder.layers.5.layer_norms.2.bias\", \"recipe_decoder.layers.6.self_attn.in_proj_weight\", \"recipe_decoder.layers.6.self_attn.in_proj_bias\", \"recipe_decoder.layers.6.self_attn.out_proj.weight\", \"recipe_decoder.layers.6.self_attn.out_proj.bias\", \"recipe_decoder.layers.6.cond_att.in_proj_weight\", \"recipe_decoder.layers.6.cond_att.in_proj_bias\", \"recipe_decoder.layers.6.cond_att.out_proj.weight\", \"recipe_decoder.layers.6.cond_att.out_proj.bias\", \"recipe_decoder.layers.6.fc1.weight\", \"recipe_decoder.layers.6.fc1.bias\", \"recipe_decoder.layers.6.fc2.weight\", \"recipe_decoder.layers.6.fc2.bias\", \"recipe_decoder.layers.6.layer_norms.0.weight\", \"recipe_decoder.layers.6.layer_norms.0.bias\", \"recipe_decoder.layers.6.layer_norms.1.weight\", \"recipe_decoder.layers.6.layer_norms.1.bias\", \"recipe_decoder.layers.6.layer_norms.2.weight\", \"recipe_decoder.layers.6.layer_norms.2.bias\", \"recipe_decoder.layers.7.self_attn.in_proj_weight\", \"recipe_decoder.layers.7.self_attn.in_proj_bias\", \"recipe_decoder.layers.7.self_attn.out_proj.weight\", \"recipe_decoder.layers.7.self_attn.out_proj.bias\", \"recipe_decoder.layers.7.cond_att.in_proj_weight\", \"recipe_decoder.layers.7.cond_att.in_proj_bias\", \"recipe_decoder.layers.7.cond_att.out_proj.weight\", \"recipe_decoder.layers.7.cond_att.out_proj.bias\", \"recipe_decoder.layers.7.fc1.weight\", \"recipe_decoder.layers.7.fc1.bias\", \"recipe_decoder.layers.7.fc2.weight\", \"recipe_decoder.layers.7.fc2.bias\", \"recipe_decoder.layers.7.layer_norms.0.weight\", \"recipe_decoder.layers.7.layer_norms.0.bias\", \"recipe_decoder.layers.7.layer_norms.1.weight\", \"recipe_decoder.layers.7.layer_norms.1.bias\", \"recipe_decoder.layers.7.layer_norms.2.weight\", \"recipe_decoder.layers.7.layer_norms.2.bias\", \"recipe_decoder.layers.8.self_attn.in_proj_weight\", \"recipe_decoder.layers.8.self_attn.in_proj_bias\", \"recipe_decoder.layers.8.self_attn.out_proj.weight\", \"recipe_decoder.layers.8.self_attn.out_proj.bias\", \"recipe_decoder.layers.8.cond_att.in_proj_weight\", \"recipe_decoder.layers.8.cond_att.in_proj_bias\", \"recipe_decoder.layers.8.cond_att.out_proj.weight\", \"recipe_decoder.layers.8.cond_att.out_proj.bias\", \"recipe_decoder.layers.8.fc1.weight\", \"recipe_decoder.layers.8.fc1.bias\", \"recipe_decoder.layers.8.fc2.weight\", \"recipe_decoder.layers.8.fc2.bias\", \"recipe_decoder.layers.8.layer_norms.0.weight\", \"recipe_decoder.layers.8.layer_norms.0.bias\", \"recipe_decoder.layers.8.layer_norms.1.weight\", \"recipe_decoder.layers.8.layer_norms.1.bias\", \"recipe_decoder.layers.8.layer_norms.2.weight\", \"recipe_decoder.layers.8.layer_norms.2.bias\", \"recipe_decoder.layers.9.self_attn.in_proj_weight\", \"recipe_decoder.layers.9.self_attn.in_proj_bias\", \"recipe_decoder.layers.9.self_attn.out_proj.weight\", \"recipe_decoder.layers.9.self_attn.out_proj.bias\", \"recipe_decoder.layers.9.cond_att.in_proj_weight\", \"recipe_decoder.layers.9.cond_att.in_proj_bias\", \"recipe_decoder.layers.9.cond_att.out_proj.weight\", \"recipe_decoder.layers.9.cond_att.out_proj.bias\", \"recipe_decoder.layers.9.fc1.weight\", \"recipe_decoder.layers.9.fc1.bias\", \"recipe_decoder.layers.9.fc2.weight\", \"recipe_decoder.layers.9.fc2.bias\", \"recipe_decoder.layers.9.layer_norms.0.weight\", \"recipe_decoder.layers.9.layer_norms.0.bias\", \"recipe_decoder.layers.9.layer_norms.1.weight\", \"recipe_decoder.layers.9.layer_norms.1.bias\", \"recipe_decoder.layers.9.layer_norms.2.weight\", \"recipe_decoder.layers.9.layer_norms.2.bias\", \"recipe_decoder.layers.10.self_attn.in_proj_weight\", \"recipe_decoder.layers.10.self_attn.in_proj_bias\", \"recipe_decoder.layers.10.self_attn.out_proj.weight\", \"recipe_decoder.layers.10.self_attn.out_proj.bias\", \"recipe_decoder.layers.10.cond_att.in_proj_weight\", \"recipe_decoder.layers.10.cond_att.in_proj_bias\", \"recipe_decoder.layers.10.cond_att.out_proj.weight\", \"recipe_decoder.layers.10.cond_att.out_proj.bias\", \"recipe_decoder.layers.10.fc1.weight\", \"recipe_decoder.layers.10.fc1.bias\", \"recipe_decoder.layers.10.fc2.weight\", \"recipe_decoder.layers.10.fc2.bias\", \"recipe_decoder.layers.10.layer_norms.0.weight\", \"recipe_decoder.layers.10.layer_norms.0.bias\", \"recipe_decoder.layers.10.layer_norms.1.weight\", \"recipe_decoder.layers.10.layer_norms.1.bias\", \"recipe_decoder.layers.10.layer_norms.2.weight\", \"recipe_decoder.layers.10.layer_norms.2.bias\", \"recipe_decoder.layers.11.self_attn.in_proj_weight\", \"recipe_decoder.layers.11.self_attn.in_proj_bias\", \"recipe_decoder.layers.11.self_attn.out_proj.weight\", \"recipe_decoder.layers.11.self_attn.out_proj.bias\", \"recipe_decoder.layers.11.cond_att.in_proj_weight\", \"recipe_decoder.layers.11.cond_att.in_proj_bias\", \"recipe_decoder.layers.11.cond_att.out_proj.weight\", \"recipe_decoder.layers.11.cond_att.out_proj.bias\", \"recipe_decoder.layers.11.fc1.weight\", \"recipe_decoder.layers.11.fc1.bias\", \"recipe_decoder.layers.11.fc2.weight\", \"recipe_decoder.layers.11.fc2.bias\", \"recipe_decoder.layers.11.layer_norms.0.weight\", \"recipe_decoder.layers.11.layer_norms.0.bias\", \"recipe_decoder.layers.11.layer_norms.1.weight\", \"recipe_decoder.layers.11.layer_norms.1.bias\", \"recipe_decoder.layers.11.layer_norms.2.weight\", \"recipe_decoder.layers.11.layer_norms.2.bias\", \"recipe_decoder.layers.12.self_attn.in_proj_weight\", \"recipe_decoder.layers.12.self_attn.in_proj_bias\", \"recipe_decoder.layers.12.self_attn.out_proj.weight\", \"recipe_decoder.layers.12.self_attn.out_proj.bias\", \"recipe_decoder.layers.12.cond_att.in_proj_weight\", \"recipe_decoder.layers.12.cond_att.in_proj_bias\", \"recipe_decoder.layers.12.cond_att.out_proj.weight\", \"recipe_decoder.layers.12.cond_att.out_proj.bias\", \"recipe_decoder.layers.12.fc1.weight\", \"recipe_decoder.layers.12.fc1.bias\", \"recipe_decoder.layers.12.fc2.weight\", \"recipe_decoder.layers.12.fc2.bias\", \"recipe_decoder.layers.12.layer_norms.0.weight\", \"recipe_decoder.layers.12.layer_norms.0.bias\", \"recipe_decoder.layers.12.layer_norms.1.weight\", \"recipe_decoder.layers.12.layer_norms.1.bias\", \"recipe_decoder.layers.12.layer_norms.2.weight\", \"recipe_decoder.layers.12.layer_norms.2.bias\", \"recipe_decoder.layers.13.self_attn.in_proj_weight\", \"recipe_decoder.layers.13.self_attn.in_proj_bias\", \"recipe_decoder.layers.13.self_attn.out_proj.weight\", \"recipe_decoder.layers.13.self_attn.out_proj.bias\", \"recipe_decoder.layers.13.cond_att.in_proj_weight\", \"recipe_decoder.layers.13.cond_att.in_proj_bias\", \"recipe_decoder.layers.13.cond_att.out_proj.weight\", \"recipe_decoder.layers.13.cond_att.out_proj.bias\", \"recipe_decoder.layers.13.fc1.weight\", \"recipe_decoder.layers.13.fc1.bias\", \"recipe_decoder.layers.13.fc2.weight\", \"recipe_decoder.layers.13.fc2.bias\", \"recipe_decoder.layers.13.layer_norms.0.weight\", \"recipe_decoder.layers.13.layer_norms.0.bias\", \"recipe_decoder.layers.13.layer_norms.1.weight\", \"recipe_decoder.layers.13.layer_norms.1.bias\", \"recipe_decoder.layers.13.layer_norms.2.weight\", \"recipe_decoder.layers.13.layer_norms.2.bias\", \"recipe_decoder.layers.14.self_attn.in_proj_weight\", \"recipe_decoder.layers.14.self_attn.in_proj_bias\", \"recipe_decoder.layers.14.self_attn.out_proj.weight\", \"recipe_decoder.layers.14.self_attn.out_proj.bias\", \"recipe_decoder.layers.14.cond_att.in_proj_weight\", \"recipe_decoder.layers.14.cond_att.in_proj_bias\", \"recipe_decoder.layers.14.cond_att.out_proj.weight\", \"recipe_decoder.layers.14.cond_att.out_proj.bias\", \"recipe_decoder.layers.14.fc1.weight\", \"recipe_decoder.layers.14.fc1.bias\", \"recipe_decoder.layers.14.fc2.weight\", \"recipe_decoder.layers.14.fc2.bias\", \"recipe_decoder.layers.14.layer_norms.0.weight\", \"recipe_decoder.layers.14.layer_norms.0.bias\", \"recipe_decoder.layers.14.layer_norms.1.weight\", \"recipe_decoder.layers.14.layer_norms.1.bias\", \"recipe_decoder.layers.14.layer_norms.2.weight\", \"recipe_decoder.layers.14.layer_norms.2.bias\", \"recipe_decoder.layers.15.self_attn.in_proj_weight\", \"recipe_decoder.layers.15.self_attn.in_proj_bias\", \"recipe_decoder.layers.15.self_attn.out_proj.weight\", \"recipe_decoder.layers.15.self_attn.out_proj.bias\", \"recipe_decoder.layers.15.cond_att.in_proj_weight\", \"recipe_decoder.layers.15.cond_att.in_proj_bias\", \"recipe_decoder.layers.15.cond_att.out_proj.weight\", \"recipe_decoder.layers.15.cond_att.out_proj.bias\", \"recipe_decoder.layers.15.fc1.weight\", \"recipe_decoder.layers.15.fc1.bias\", \"recipe_decoder.layers.15.fc2.weight\", \"recipe_decoder.layers.15.fc2.bias\", \"recipe_decoder.layers.15.layer_norms.0.weight\", \"recipe_decoder.layers.15.layer_norms.0.bias\", \"recipe_decoder.layers.15.layer_norms.1.weight\", \"recipe_decoder.layers.15.layer_norms.1.bias\", \"recipe_decoder.layers.15.layer_norms.2.weight\", \"recipe_decoder.layers.15.layer_norms.2.bias\", \"recipe_decoder.linear.weight\", \"recipe_decoder.linear.bias\", \"image_encoder.vit.embeddings.cls_token\", \"image_encoder.vit.embeddings.position_embeddings\", \"image_encoder.vit.embeddings.patch_embeddings.projection.weight\", \"image_encoder.vit.embeddings.patch_embeddings.projection.bias\", \"image_encoder.vit.encoder.layer.0.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.0.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.0.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.0.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.0.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.0.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.0.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.0.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.0.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.0.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.0.output.dense.weight\", \"image_encoder.vit.encoder.layer.0.output.dense.bias\", \"image_encoder.vit.encoder.layer.0.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.0.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.0.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.0.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.1.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.1.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.1.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.1.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.1.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.1.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.1.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.1.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.1.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.1.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.1.output.dense.weight\", \"image_encoder.vit.encoder.layer.1.output.dense.bias\", \"image_encoder.vit.encoder.layer.1.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.1.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.1.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.1.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.2.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.2.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.2.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.2.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.2.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.2.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.2.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.2.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.2.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.2.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.2.output.dense.weight\", \"image_encoder.vit.encoder.layer.2.output.dense.bias\", \"image_encoder.vit.encoder.layer.2.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.2.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.2.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.2.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.3.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.3.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.3.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.3.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.3.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.3.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.3.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.3.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.3.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.3.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.3.output.dense.weight\", \"image_encoder.vit.encoder.layer.3.output.dense.bias\", \"image_encoder.vit.encoder.layer.3.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.3.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.3.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.3.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.4.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.4.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.4.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.4.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.4.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.4.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.4.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.4.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.4.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.4.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.4.output.dense.weight\", \"image_encoder.vit.encoder.layer.4.output.dense.bias\", \"image_encoder.vit.encoder.layer.4.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.4.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.4.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.4.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.5.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.5.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.5.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.5.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.5.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.5.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.5.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.5.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.5.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.5.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.5.output.dense.weight\", \"image_encoder.vit.encoder.layer.5.output.dense.bias\", \"image_encoder.vit.encoder.layer.5.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.5.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.5.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.5.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.6.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.6.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.6.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.6.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.6.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.6.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.6.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.6.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.6.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.6.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.6.output.dense.weight\", \"image_encoder.vit.encoder.layer.6.output.dense.bias\", \"image_encoder.vit.encoder.layer.6.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.6.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.6.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.6.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.7.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.7.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.7.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.7.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.7.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.7.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.7.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.7.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.7.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.7.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.7.output.dense.weight\", \"image_encoder.vit.encoder.layer.7.output.dense.bias\", \"image_encoder.vit.encoder.layer.7.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.7.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.7.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.7.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.8.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.8.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.8.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.8.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.8.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.8.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.8.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.8.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.8.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.8.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.8.output.dense.weight\", \"image_encoder.vit.encoder.layer.8.output.dense.bias\", \"image_encoder.vit.encoder.layer.8.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.8.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.8.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.8.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.9.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.9.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.9.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.9.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.9.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.9.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.9.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.9.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.9.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.9.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.9.output.dense.weight\", \"image_encoder.vit.encoder.layer.9.output.dense.bias\", \"image_encoder.vit.encoder.layer.9.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.9.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.9.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.9.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.10.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.10.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.10.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.10.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.10.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.10.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.10.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.10.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.10.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.10.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.10.output.dense.weight\", \"image_encoder.vit.encoder.layer.10.output.dense.bias\", \"image_encoder.vit.encoder.layer.10.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.10.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.10.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.10.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.11.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.11.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.11.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.11.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.11.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.11.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.11.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.11.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.11.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.11.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.11.output.dense.weight\", \"image_encoder.vit.encoder.layer.11.output.dense.bias\", \"image_encoder.vit.encoder.layer.11.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.11.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.11.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.11.layernorm_after.bias\", \"image_encoder.vit.layernorm.weight\", \"image_encoder.vit.layernorm.bias\", \"image_encoder.vit.pooler.dense.weight\", \"image_encoder.vit.pooler.dense.bias\", \"image_encoder.linear.weight\", \"image_encoder.linear.bias\", \"image_encoder.sequential.0.weight\", \"image_encoder.sequential.0.bias\", \"ingredient_decoder.embed_tokens.weight\", \"ingredient_decoder.layer_norms_in.0.weight\", \"ingredient_decoder.layer_norms_in.0.bias\", \"ingredient_decoder.layer_norms_in.1.weight\", \"ingredient_decoder.layer_norms_in.1.bias\", \"ingredient_decoder.layer_norms_in.2.weight\", \"ingredient_decoder.layer_norms_in.2.bias\", \"ingredient_decoder.layers.0.self_attn.in_proj_weight\", \"ingredient_decoder.layers.0.self_attn.in_proj_bias\", \"ingredient_decoder.layers.0.self_attn.out_proj.weight\", \"ingredient_decoder.layers.0.self_attn.out_proj.bias\", \"ingredient_decoder.layers.0.cond_att.in_proj_weight\", \"ingredient_decoder.layers.0.cond_att.in_proj_bias\", \"ingredient_decoder.layers.0.cond_att.out_proj.weight\", \"ingredient_decoder.layers.0.cond_att.out_proj.bias\", \"ingredient_decoder.layers.0.fc1.weight\", \"ingredient_decoder.layers.0.fc1.bias\", \"ingredient_decoder.layers.0.fc2.weight\", \"ingredient_decoder.layers.0.fc2.bias\", \"ingredient_decoder.layers.0.layer_norms.0.weight\", \"ingredient_decoder.layers.0.layer_norms.0.bias\", \"ingredient_decoder.layers.0.layer_norms.1.weight\", \"ingredient_decoder.layers.0.layer_norms.1.bias\", \"ingredient_decoder.layers.0.layer_norms.2.weight\", \"ingredient_decoder.layers.0.layer_norms.2.bias\", \"ingredient_decoder.layers.0.last_ln.weight\", \"ingredient_decoder.layers.0.last_ln.bias\", \"ingredient_decoder.layers.1.self_attn.in_proj_weight\", \"ingredient_decoder.layers.1.self_attn.in_proj_bias\", \"ingredient_decoder.layers.1.self_attn.out_proj.weight\", \"ingredient_decoder.layers.1.self_attn.out_proj.bias\", \"ingredient_decoder.layers.1.cond_att.in_proj_weight\", \"ingredient_decoder.layers.1.cond_att.in_proj_bias\", \"ingredient_decoder.layers.1.cond_att.out_proj.weight\", \"ingredient_decoder.layers.1.cond_att.out_proj.bias\", \"ingredient_decoder.layers.1.fc1.weight\", \"ingredient_decoder.layers.1.fc1.bias\", \"ingredient_decoder.layers.1.fc2.weight\", \"ingredient_decoder.layers.1.fc2.bias\", \"ingredient_decoder.layers.1.layer_norms.0.weight\", \"ingredient_decoder.layers.1.layer_norms.0.bias\", \"ingredient_decoder.layers.1.layer_norms.1.weight\", \"ingredient_decoder.layers.1.layer_norms.1.bias\", \"ingredient_decoder.layers.1.layer_norms.2.weight\", \"ingredient_decoder.layers.1.layer_norms.2.bias\", \"ingredient_decoder.layers.1.last_ln.weight\", \"ingredient_decoder.layers.1.last_ln.bias\", \"ingredient_decoder.layers.2.self_attn.in_proj_weight\", \"ingredient_decoder.layers.2.self_attn.in_proj_bias\", \"ingredient_decoder.layers.2.self_attn.out_proj.weight\", \"ingredient_decoder.layers.2.self_attn.out_proj.bias\", \"ingredient_decoder.layers.2.cond_att.in_proj_weight\", \"ingredient_decoder.layers.2.cond_att.in_proj_bias\", \"ingredient_decoder.layers.2.cond_att.out_proj.weight\", \"ingredient_decoder.layers.2.cond_att.out_proj.bias\", \"ingredient_decoder.layers.2.fc1.weight\", \"ingredient_decoder.layers.2.fc1.bias\", \"ingredient_decoder.layers.2.fc2.weight\", \"ingredient_decoder.layers.2.fc2.bias\", \"ingredient_decoder.layers.2.layer_norms.0.weight\", \"ingredient_decoder.layers.2.layer_norms.0.bias\", \"ingredient_decoder.layers.2.layer_norms.1.weight\", \"ingredient_decoder.layers.2.layer_norms.1.bias\", \"ingredient_decoder.layers.2.layer_norms.2.weight\", \"ingredient_decoder.layers.2.layer_norms.2.bias\", \"ingredient_decoder.layers.2.last_ln.weight\", \"ingredient_decoder.layers.2.last_ln.bias\", \"ingredient_decoder.layers.3.self_attn.in_proj_weight\", \"ingredient_decoder.layers.3.self_attn.in_proj_bias\", \"ingredient_decoder.layers.3.self_attn.out_proj.weight\", \"ingredient_decoder.layers.3.self_attn.out_proj.bias\", \"ingredient_decoder.layers.3.cond_att.in_proj_weight\", \"ingredient_decoder.layers.3.cond_att.in_proj_bias\", \"ingredient_decoder.layers.3.cond_att.out_proj.weight\", \"ingredient_decoder.layers.3.cond_att.out_proj.bias\", \"ingredient_decoder.layers.3.fc1.weight\", \"ingredient_decoder.layers.3.fc1.bias\", \"ingredient_decoder.layers.3.fc2.weight\", \"ingredient_decoder.layers.3.fc2.bias\", \"ingredient_decoder.layers.3.layer_norms.0.weight\", \"ingredient_decoder.layers.3.layer_norms.0.bias\", \"ingredient_decoder.layers.3.layer_norms.1.weight\", \"ingredient_decoder.layers.3.layer_norms.1.bias\", \"ingredient_decoder.layers.3.layer_norms.2.weight\", \"ingredient_decoder.layers.3.layer_norms.2.bias\", \"ingredient_decoder.layers.3.last_ln.weight\", \"ingredient_decoder.layers.3.last_ln.bias\", \"ingredient_decoder.linear.weight\", \"ingredient_decoder.linear.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d8abfe1a8c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_vit_state_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1671\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1672\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EncoderVisionTransformer:\n\tMissing key(s) in state_dict: \"vit.embeddings.cls_token\", \"vit.embeddings.position_embeddings\", \"vit.embeddings.patch_embeddings.projection.weight\", \"vit.embeddings.patch_embeddings.projection.bias\", \"vit.encoder.layer.0.attention.attention.query.weight\", \"vit.encoder.layer.0.attention.attention.query.bias\", \"vit.encoder.layer.0.attention.attention.key.weight\", \"vit.encoder.layer.0.attention.attention.key.bias\", \"vit.encoder.layer.0.attention.attention.value.weight\", \"vit.encoder.layer.0.attention.attention.value.bias\", \"vit.encoder.layer.0.attention.output.dense.weight\", \"vit.encoder.layer.0.attention.output.dense.bias\", \"vit.encoder.layer.0.intermediate.dense.weight\", \"vit.encoder.layer.0.intermediate.dense.bias\", \"vit.encoder.layer.0.output.dense.weight\", \"vit.encoder.layer.0.output.dense.bias\", \"vit.encoder.layer.0.layernorm_before.weight\", \"vit.encoder.layer.0.layernorm_before.bias\", \"vit.encoder.layer.0.layernorm_after.weight\", \"vit.encoder.layer.0.layernorm_after.bias\", \"vit.encoder.layer.1.attention.attention.query.weight\", \"vit.encoder.layer.1.attention.attention.query.bias\", \"vit.encoder.layer.1.attention.attention.key.weight\", \"vit.encoder.layer.1.attention.attention.key.bias\", \"vit.encoder.layer.1.attention.attention.value.weight\", \"vit.encoder.layer.1.attention.attention.value.bias\", \"vit.encoder.layer.1.attention.output.dense.weight\", \"vit.encoder.layer.1.attention.output.dense.bias\", \"vit.encoder.layer.1.intermediate.dense.weight\", \"vit.encoder.layer.1.intermediate.dense.bias\", \"vit.encoder.layer.1.output.dense.weight\", \"vit.encoder.layer.1.output.dense.bias\", \"vit.encoder.layer.1.layernorm_before.weight\", \"vit.encoder.layer.1.layernorm_before.bias\", \"vit.encoder.layer.1.layernorm_after.weight\", \"vit.encoder.layer.1.layernorm_after.bias\", \"vit.encoder.layer.2.attention.attention.query.weight\", \"vit.encoder.layer.2.attention.attention.query.bias\", \"vit.encoder.layer.2.attention.attention.key.weight\", \"vit.encoder.layer.2.attention.attention.key.bias\", \"vit.encoder.layer.2.attention.attention.value.weight\", \"vit.encoder.layer.2.attention.attention.value.bias\", \"vit.encoder.layer.2.attention.output.dense.weight\", \"vit.encoder.layer.2.attention.output.dense.bias\", \"vit.encoder.layer.2.intermediate.dense.weight\", \"vit.encoder.layer.2.intermediate.dense.bias\", \"vit.encoder.layer.2.output.dense.weight\", \"vit.encoder.layer.2.output.dense.bias\", \"vit.encoder.layer.2.layernorm_before.weight\", \"vit.encoder.layer.2.layernorm_before.bias\", \"vit.encoder.layer.2.layernorm_after.weight\", \"vit.encoder.layer.2.layernorm_after.bias\", \"vit.encoder.layer.3.attention.attention.query.weight\", \"vit.encoder.layer.3.attention.attention.query.bias\", \"vit.encoder.layer.3.attention.attention.key.weight\", \"vit.encoder.layer.3.attention.attention.key.bias\", \"vit.encoder.layer.3.attention.attention.value.weight\", \"vit.encoder.layer.3.attention.attention.value.bias\", \"vit.encoder.layer.3.attention.output.dense.weight\", \"vit.encoder.layer.3.attention.output.dense.bias\", \"vit.encoder.layer.3.intermediate.dense.weight\", \"vit.encoder.layer.3.intermediate.dense.bias\", \"vit.encoder.layer.3.output.dense.weight\", \"vit.encoder.layer.3.output.dense.bias\", \"vit.encoder.layer.3.layernorm_before.weight\", \"vit.encoder.layer.3.layernorm_before.bias\", \"vit.encoder.layer.3.layernorm_after.weight\", \"vit.encoder.layer.3.layernorm_after.bias\", \"vit.encoder.layer.4.attention.attention.query.weight\", \"vit.encoder.layer.4.attention.attention.query.bias\", \"vit.encoder.layer.4.attention.attention.key.weight\", \"vit.encoder.layer.4.attention.attention.key.bias\", \"vit.encoder.layer.4.attention.attention.value.weight\", \"vit.encoder.layer.4.attention.attention.value.bias\", \"vit.encoder.layer.4.attention.output.dense.weight\", \"vit.encoder.layer.4.attention.output.dense.bias\", \"vit.encoder.layer.4.intermediate.dense.weight\", \"vit.encoder.layer.4.intermediate.dense.bias\", \"vit.encoder.layer.4.output.dense.weight\", \"vit.encoder.layer.4.output.dense.bias\", \"vit.encoder.layer.4.layernorm_before.weight\", \"vit.encoder.layer.4.layernorm_before.bias\", \"vit.encoder.layer.4.layernorm_after.weight\", \"vit.encoder.layer.4.layernorm_after.bias\", \"vit.encoder.layer.5.attention.attention.query.weight\", \"vit.encoder.layer.5.attention.attention.query.bias\", \"vit.encoder.layer.5.attention.attention.key.weight\", \"vit.encoder.layer.5.attention.attention.key.bias\", \"vit.encoder.layer.5.attention.attention.value.weight\", \"vit.encoder.layer.5.attention.attention.value.bias\", \"vit.encoder.layer.5.attention.output.dense.weight\", \"vit.encoder.layer.5.attention.output.dense.bias\", \"vit.encoder.layer.5.intermediate.dense.weight\", \"vit.encoder.layer.5.intermediate.dense.bias\", \"vit.encoder.layer.5.output.dense.weight\", \"vit.encoder.layer.5.output.dense.bias\", \"vit.encoder.layer.5.layernorm_before.weight\", \"vit.encoder.layer.5.layernorm_before.bias\", \"vit.encoder.layer.5.layernorm_after.weight\", \"vit.encoder.layer.5.layernorm_after.bias\", \"vit.encoder.layer.6.attention.attention.query.weight\", \"vit.encoder.layer.6.attention.attention.query.bias\", \"vit.encoder.layer.6.attention.attention.key.weight\", \"vit.encoder.layer.6.attention.attention.key.bias\", \"vit.encoder.layer.6.attention.attention.value.weight\", \"vit.encoder.layer.6.attention.attention.value.bias\", \"vit.encoder.layer.6.attention.output.dense.weight\", \"vit.encoder.layer.6.attention.output.dense.bias\", \"vit.encoder.layer.6.intermediate.dense.weight\", \"vit.encoder.layer.6.intermediate.dense.bias\", \"vit.encoder.layer.6.output.dense.weight\", \"vit.encoder.layer.6.output.dense.bias\", \"vit.encoder.layer.6.layernorm_before.weight\", \"vit.encoder.layer.6.layernorm_before.bias\", \"vit.encoder.layer.6.layernorm_after.weight\", \"vit.encoder.layer.6.layernorm_after.bias\", \"vit.encoder.layer.7.attention.attention.query.weight\", \"vit.encoder.layer.7.attention.attention.query.bias\", \"vit.encoder.layer.7.attention.attention.key.weight\", \"vit.encoder.layer.7.attention.attention.key.bias\", \"vit.encoder.layer.7.attention.attention.value.weight\", \"vit.encoder.layer.7.attention.attention.value.bias\", \"vit.encoder.layer.7.attention.output.dense.weight\", \"vit.encoder.layer.7.attention.output.dense.bias\", \"vit.encoder.layer.7.intermediate.dense.weight\", \"vit.encoder.layer.7.intermediate.dense.bias\", \"vit.encoder.layer.7.output.dense.weight\", \"vit.encoder.layer.7.output.dense.bias\", \"vit.encoder.layer.7.layernorm_before.weight\", \"vit.encoder.layer.7.layernorm_before.bias\", \"vit.encoder.layer.7.layernorm_after.weight\", \"vit.encoder.layer.7.layernorm_after.bias\", \"vit.encoder.layer.8.attention.attention.query.weight\", \"vit.encoder.layer.8.attention.attention.query.bias\", \"vit.encoder.layer.8.attention.attention.key.weight\", \"vit.encoder.layer.8.attention.attention.key.bias\", \"vit.encoder.layer.8.attention.attention.value.weight\", \"vit.encoder.layer.8.attention.attention.value.bias\", \"vit.encoder.layer.8.attention.output.dense.weight\", \"vit.encoder.layer.8.attention.output.dense.bias\", \"vit.encoder.layer.8.intermediate.dense.weight\", \"vit.encoder.layer.8.intermediate.dense.bias\", \"vit.encoder.layer.8.output.dense.weight\", \"vit.encoder.layer.8.output.dense.bias\", \"vit.encoder.layer.8.layernorm_before.weight\", \"vit.encoder.layer.8.layernorm_before.bias\", \"vit.encoder.layer.8.layernorm_after.weight\", \"vit.encoder.layer.8.layernorm_after.bias\", \"vit.encoder.layer.9.attention.attention.query.weight\", \"vit.encoder.layer.9.attention.attention.query.bias\", \"vit.encoder.layer.9.attention.attention.key.weight\", \"vit.encoder.layer.9.attention.attention.key.bias\", \"vit.encoder.layer.9.attention.attention.value.weight\", \"vit.encoder.layer.9.attention.attention.value.bias\", \"vit.encoder.layer.9.attention.output.dense.weight\", \"vit.encoder.layer.9.attention.output.dense.bias\", \"vit.encoder.layer.9.intermediate.dense.weight\", \"vit.encoder.layer.9.intermediate.dense.bias\", \"vit.encoder.layer.9.output.dense.weight\", \"vit.encoder.layer.9.output.dense.bias\", \"vit.encoder.layer.9.layernorm_before.weight\", \"vit.encoder.layer.9.layernorm_before.bias\", \"vit.encoder.layer.9.layernorm_after.weight\", \"vit.encoder.layer.9.layernorm_after.bias\", \"vit.encoder.layer.10.attention.attention.query.weight\", \"vit.encoder.layer.10.attention.attention.query.bias\", \"vit.encoder.layer.10.attention.attention.key.weight\", \"vit.encoder.layer.10.attention.attention.key.bias\", \"vit.encoder.layer.10.attention.attention.value.weight\", \"vit.encoder.layer.10.attention.attention.value.bias\", \"vit.encoder.layer.10.attention.output.dense.weight\", \"vit.encoder.layer.10.attention.output.dense.bias\", \"vit.encoder.layer.10.intermediate.dense.weight\", \"vit.encoder.layer.10.intermediate.dense.bias\", \"vit.encoder.layer.10.output.dense.weight\", \"vit.encoder.layer.10.output.dense.bias\", \"vit.encoder.layer.10.layernorm_before.weight\", \"vit.encoder.layer.10.layernorm_before.bias\", \"vit.encoder.layer.10.layernorm_after.weight\", \"vit.encoder.layer.10.layernorm_after.bias\", \"vit.encoder.layer.11.attention.attention.query.weight\", \"vit.encoder.layer.11.attention.attention.query.bias\", \"vit.encoder.layer.11.attention.attention.key.weight\", \"vit.encoder.layer.11.attention.attention.key.bias\", \"vit.encoder.layer.11.attention.attention.value.weight\", \"vit.encoder.layer.11.attention.attention.value.bias\", \"vit.encoder.layer.11.attention.output.dense.weight\", \"vit.encoder.layer.11.attention.output.dense.bias\", \"vit.encoder.layer.11.intermediate.dense.weight\", \"vit.encoder.layer.11.intermediate.dense.bias\", \"vit.encoder.layer.11.output.dense.weight\", \"vit.encoder.layer.11.output.dense.bias\", \"vit.encoder.layer.11.layernorm_before.weight\", \"vit.encoder.layer.11.layernorm_before.bias\", \"vit.encoder.layer.11.layernorm_after.weight\", \"vit.encoder.layer.11.layernorm_after.bias\", \"vit.layernorm.weight\", \"vit.layernorm.bias\", \"vit.pooler.dense.weight\", \"vit.pooler.dense.bias\", \"linear.weight\", \"linear.bias\", \"sequential.0.weight\", \"sequential.0.bias\". \n\tUnexpected key(s) in state_dict: \"ingredient_encoder.linear.weight\", \"recipe_decoder.embed_tokens.weight\", \"recipe_decoder.embed_positions.weight\", \"recipe_decoder.layers.0.self_attn.in_proj_weight\", \"recipe_decoder.layers.0.self_attn.in_proj_bias\", \"recipe_decoder.layers.0.self_attn.out_proj.weight\", \"recipe_decoder.layers.0.self_attn.out_proj.bias\", \"recipe_decoder.layers.0.cond_att.in_proj_weight\", \"recipe_decoder.layers.0.cond_att.in_proj_bias\", \"recipe_decoder.layers.0.cond_att.out_proj.weight\", \"recipe_decoder.layers.0.cond_att.out_proj.bias\", \"recipe_decoder.layers.0.fc1.weight\", \"recipe_decoder.layers.0.fc1.bias\", \"recipe_decoder.layers.0.fc2.weight\", \"recipe_decoder.layers.0.fc2.bias\", \"recipe_decoder.layers.0.layer_norms.0.weight\", \"recipe_decoder.layers.0.layer_norms.0.bias\", \"recipe_decoder.layers.0.layer_norms.1.weight\", \"recipe_decoder.layers.0.layer_norms.1.bias\", \"recipe_decoder.layers.0.layer_norms.2.weight\", \"recipe_decoder.layers.0.layer_norms.2.bias\", \"recipe_decoder.layers.1.self_attn.in_proj_weight\", \"recipe_decoder.layers.1.self_attn.in_proj_bias\", \"recipe_decoder.layers.1.self_attn.out_proj.weight\", \"recipe_decoder.layers.1.self_attn.out_proj.bias\", \"recipe_decoder.layers.1.cond_att.in_proj_weight\", \"recipe_decoder.layers.1.cond_att.in_proj_bias\", \"recipe_decoder.layers.1.cond_att.out_proj.weight\", \"recipe_decoder.layers.1.cond_att.out_proj.bias\", \"recipe_decoder.layers.1.fc1.weight\", \"recipe_decoder.layers.1.fc1.bias\", \"recipe_decoder.layers.1.fc2.weight\", \"recipe_decoder.layers.1.fc2.bias\", \"recipe_decoder.layers.1.layer_norms.0.weight\", \"recipe_decoder.layers.1.layer_norms.0.bias\", \"recipe_decoder.layers.1.layer_norms.1.weight\", \"recipe_decoder.layers.1.layer_norms.1.bias\", \"recipe_decoder.layers.1.layer_norms.2.weight\", \"recipe_decoder.layers.1.layer_norms.2.bias\", \"recipe_decoder.layers.2.self_attn.in_proj_weight\", \"recipe_decoder.layers.2.self_attn.in_proj_bias\", \"recipe_decoder.layers.2.self_attn.out_proj.weight\", \"recipe_decoder.layers.2.self_attn.out_proj.bias\", \"recipe_decoder.layers.2.cond_att.in_proj_weight\", \"recipe_decoder.layers.2.cond_att.in_proj_bias\", \"recipe_decoder.layers.2.cond_att.out_proj.weight\", \"recipe_decoder.layers.2.cond_att.out_proj.bias\", \"recipe_decoder.layers.2.fc1.weight\", \"recipe_decoder.layers.2.fc1.bias\", \"recipe_decoder.layers.2.fc2.weight\", \"recipe_decoder.layers.2.fc2.bias\", \"recipe_decoder.layers.2.layer_norms.0.weight\", \"recipe_decoder.layers.2.layer_norms.0.bias\", \"recipe_decoder.layers.2.layer_norms.1.weight\", \"recipe_decoder.layers.2.layer_norms.1.bias\", \"recipe_decoder.layers.2.layer_norms.2.weight\", \"recipe_decoder.layers.2.layer_norms.2.bias\", \"recipe_decoder.layers.3.self_attn.in_proj_weight\", \"recipe_decoder.layers.3.self_attn.in_proj_bias\", \"recipe_decoder.layers.3.self_attn.out_proj.weight\", \"recipe_decoder.layers.3.self_attn.out_proj.bias\", \"recipe_decoder.layers.3.cond_att.in_proj_weight\", \"recipe_decoder.layers.3.cond_att.in_proj_bias\", \"recipe_decoder.layers.3.cond_att.out_proj.weight\", \"recipe_decoder.layers.3.cond_att.out_proj.bias\", \"recipe_decoder.layers.3.fc1.weight\", \"recipe_decoder.layers.3.fc1.bias\", \"recipe_decoder.layers.3.fc2.weight\", \"recipe_decoder.layers.3.fc2.bias\", \"recipe_decoder.layers.3.layer_norms.0.weight\", \"recipe_decoder.layers.3.layer_norms.0.bias\", \"recipe_decoder.layers.3.layer_norms.1.weight\", \"recipe_decoder.layers.3.layer_norms.1.bias\", \"recipe_decoder.layers.3.layer_norms.2.weight\", \"recipe_decoder.layers.3.layer_norms.2.bias\", \"recipe_decoder.layers.4.self_attn.in_proj_weight\", \"recipe_decoder.layers.4.self_attn.in_proj_bias\", \"recipe_decoder.layers.4.self_attn.out_proj.weight\", \"recipe_decoder.layers.4.self_attn.out_proj.bias\", \"recipe_decoder.layers.4.cond_att.in_proj_weight\", \"recipe_decoder.layers.4.cond_att.in_proj_bias\", \"recipe_decoder.layers.4.cond_att.out_proj.weight\", \"recipe_decoder.layers.4.cond_att.out_proj.bias\", \"recipe_decoder.layers.4.fc1.weight\", \"recipe_decoder.layers.4.fc1.bias\", \"recipe_decoder.layers.4.fc2.weight\", \"recipe_decoder.layers.4.fc2.bias\", \"recipe_decoder.layers.4.layer_norms.0.weight\", \"recipe_decoder.layers.4.layer_norms.0.bias\", \"recipe_decoder.layers.4.layer_norms.1.weight\", \"recipe_decoder.layers.4.layer_norms.1.bias\", \"recipe_decoder.layers.4.layer_norms.2.weight\", \"recipe_decoder.layers.4.layer_norms.2.bias\", \"recipe_decoder.layers.5.self_attn.in_proj_weight\", \"recipe_decoder.layers.5.self_attn.in_proj_bias\", \"recipe_decoder.layers.5.self_attn.out_proj.weight\", \"recipe_decoder.layers.5.self_attn.out_proj.bias\", \"recipe_decoder.layers.5.cond_att.in_proj_weight\", \"recipe_decoder.layers.5.cond_att.in_proj_bias\", \"recipe_decoder.layers.5.cond_att.out_proj.weight\", \"recipe_decoder.layers.5.cond_att.out_proj.bias\", \"recipe_decoder.layers.5.fc1.weight\", \"recipe_decoder.layers.5.fc1.bias\", \"recipe_decoder.layers.5.fc2.weight\", \"recipe_decoder.layers.5.fc2.bias\", \"recipe_decoder.layers.5.layer_norms.0.weight\", \"recipe_decoder.layers.5.layer_norms.0.bias\", \"recipe_decoder.layers.5.layer_norms.1.weight\", \"recipe_decoder.layers.5.layer_norms.1.bias\", \"recipe_decoder.layers.5.layer_norms.2.weight\", \"recipe_decoder.layers.5.layer_norms.2.bias\", \"recipe_decoder.layers.6.self_attn.in_proj_weight\", \"recipe_decoder.layers.6.self_attn.in_proj_bias\", \"recipe_decoder.layers.6.self_attn.out_proj.weight\", \"recipe_decoder.layers.6.self_attn.out_proj.bias\", \"recipe_decoder.layers.6.cond_att.in_proj_weight\", \"recipe_decoder.layers.6.cond_att.in_proj_bias\", \"recipe_decoder.layers.6.cond_att.out_proj.weight\", \"recipe_decoder.layers.6.cond_att.out_proj.bias\", \"recipe_decoder.layers.6.fc1.weight\", \"recipe_decoder.layers.6.fc1.bias\", \"recipe_decoder.layers.6.fc2.weight\", \"recipe_decoder.layers.6.fc2.bias\", \"recipe_decoder.layers.6.layer_norms.0.weight\", \"recipe_decoder.layers.6.layer_norms.0.bias\", \"recipe_decoder.layers.6.layer_norms.1.weight\", \"recipe_decoder.layers.6.layer_norms.1.bias\", \"recipe_decoder.layers.6.layer_norms.2.weight\", \"recipe_decoder.layers.6.layer_norms.2.bias\", \"recipe_decoder.layers.7.self_attn.in_proj_weight\", \"recipe_decoder.layers.7.self_attn.in_proj_bias\", \"recipe_decoder.layers.7.self_attn.out_proj.weight\", \"recipe_decoder.layers.7.self_attn.out_proj.bias\", \"recipe_decoder.layers.7.cond_att.in_proj_weight\", \"recipe_decoder.layers.7.cond_att.in_proj_bias\", \"recipe_decoder.layers.7.cond_att.out_proj.weight\", \"recipe_decoder.layers.7.cond_att.out_proj.bias\", \"recipe_decoder.layers.7.fc1.weight\", \"recipe_decoder.layers.7.fc1.bias\", \"recipe_decoder.layers.7.fc2.weight\", \"recipe_decoder.layers.7.fc2.bias\", \"recipe_decoder.layers.7.layer_norms.0.weight\", \"recipe_decoder.layers.7.layer_norms.0.bias\", \"recipe_decoder.layers.7.layer_norms.1.weight\", \"recipe_decoder.layers.7.layer_norms.1.bias\", \"recipe_decoder.layers.7.layer_norms.2.weight\", \"recipe_decoder.layers.7.layer_norms.2.bias\", \"recipe_decoder.layers.8.self_attn.in_proj_weight\", \"recipe_decoder.layers.8.self_attn.in_proj_bias\", \"recipe_decoder.layers.8.self_attn.out_proj.weight\", \"recipe_decoder.layers.8.self_attn.out_proj.bias\", \"recipe_decoder.layers.8.cond_att.in_proj_weight\", \"recipe_decoder.layers.8.cond_att.in_proj_bias\", \"recipe_decoder.layers.8.cond_att.out_proj.weight\", \"recipe_decoder.layers.8.cond_att.out_proj.bias\", \"recipe_decoder.layers.8.fc1.weight\", \"recipe_decoder.layers.8.fc1.bias\", \"recipe_decoder.layers.8.fc2.weight\", \"recipe_decoder.layers.8.fc2.bias\", \"recipe_decoder.layers.8.layer_norms.0.weight\", \"recipe_decoder.layers.8.layer_norms.0.bias\", \"recipe_decoder.layers.8.layer_norms.1.weight\", \"recipe_decoder.layers.8.layer_norms.1.bias\", \"recipe_decoder.layers.8.layer_norms.2.weight\", \"recipe_decoder.layers.8.layer_norms.2.bias\", \"recipe_decoder.layers.9.self_attn.in_proj_weight\", \"recipe_decoder.layers.9.self_attn.in_proj_bias\", \"recipe_decoder.layers.9.self_attn.out_proj.weight\", \"recipe_decoder.layers.9.self_attn.out_proj.bias\", \"recipe_decoder.layers.9.cond_att.in_proj_weight\", \"recipe_decoder.layers.9.cond_att.in_proj_bias\", \"recipe_decoder.layers.9.cond_att.out_proj.weight\", \"recipe_decoder.layers.9.cond_att.out_proj.bias\", \"recipe_decoder.layers.9.fc1.weight\", \"recipe_decoder.layers.9.fc1.bias\", \"recipe_decoder.layers.9.fc2.weight\", \"recipe_decoder.layers.9.fc2.bias\", \"recipe_decoder.layers.9.layer_norms.0.weight\", \"recipe_decoder.layers.9.layer_norms.0.bias\", \"recipe_decoder.layers.9.layer_norms.1.weight\", \"recipe_decoder.layers.9.layer_norms.1.bias\", \"recipe_decoder.layers.9.layer_norms.2.weight\", \"recipe_decoder.layers.9.layer_norms.2.bias\", \"recipe_decoder.layers.10.self_attn.in_proj_weight\", \"recipe_decoder.layers.10.self_attn.in_proj_bias\", \"recipe_decoder.layers.10.self_attn.out_proj.weight\", \"recipe_decoder.layers.10.self_attn.out_proj.bias\", \"recipe_decoder.layers.10.cond_att.in_proj_weight\", \"recipe_decoder.layers.10.cond_att.in_proj_bias\", \"recipe_decoder.layers.10.cond_att.out_proj.weight\", \"recipe_decoder.layers.10.cond_att.out_proj.bias\", \"recipe_decoder.layers.10.fc1.weight\", \"recipe_decoder.layers.10.fc1.bias\", \"recipe_decoder.layers.10.fc2.weight\", \"recipe_decoder.layers.10.fc2.bias\", \"recipe_decoder.layers.10.layer_norms.0.weight\", \"recipe_decoder.layers.10.layer_norms.0.bias\", \"recipe_decoder.layers.10.layer_norms.1.weight\", \"recipe_decoder.layers.10.layer_norms.1.bias\", \"recipe_decoder.layers.10.layer_norms.2.weight\", \"recipe_decoder.layers.10.layer_norms.2.bias\", \"recipe_decoder.layers.11.self_attn.in_proj_weight\", \"recipe_decoder.layers.11.self_attn.in_proj_bias\", \"recipe_decoder.layers.11.self_attn.out_proj.weight\", \"recipe_decoder.layers.11.self_attn.out_proj.bias\", \"recipe_decoder.layers.11.cond_att.in_proj_weight\", \"recipe_decoder.layers.11.cond_att.in_proj_bias\", \"recipe_decoder.layers.11.cond_att.out_proj.weight\", \"recipe_decoder.layers.11.cond_att.out_proj.bias\", \"recipe_decoder.layers.11.fc1.weight\", \"recipe_decoder.layers.11.fc1.bias\", \"recipe_decoder.layers.11.fc2.weight\", \"recipe_decoder.layers.11.fc2.bias\", \"recipe_decoder.layers.11.layer_norms.0.weight\", \"recipe_decoder.layers.11.layer_norms.0.bias\", \"recipe_decoder.layers.11.layer_norms.1.weight\", \"recipe_decoder.layers.11.layer_norms.1.bias\", \"recipe_decoder.layers.11.layer_norms.2.weight\", \"recipe_decoder.layers.11.layer_norms.2.bias\", \"recipe_decoder.layers.12.self_attn.in_proj_weight\", \"recipe_decoder.layers.12.self_attn.in_proj_bias\", \"recipe_decoder.layers.12.self_attn.out_proj.weight\", \"recipe_decoder.layers.12.self_attn.out_proj.bias\", \"recipe_decoder.layers.12.cond_att.in_proj_weight\", \"recipe_decoder.layers.12.cond_att.in_proj_bias\", \"recipe_decoder.layers.12.cond_att.out_proj.weight\", \"recipe_decoder.layers.12.cond_att.out_proj.bias\", \"recipe_decoder.layers.12.fc1.weight\", \"recipe_decoder.layers.12.fc1.bias\", \"recipe_decoder.layers.12.fc2.weight\", \"recipe_decoder.layers.12.fc2.bias\", \"recipe_decoder.layers.12.layer_norms.0.weight\", \"recipe_decoder.layers.12.layer_norms.0.bias\", \"recipe_decoder.layers.12.layer_norms.1.weight\", \"recipe_decoder.layers.12.layer_norms.1.bias\", \"recipe_decoder.layers.12.layer_norms.2.weight\", \"recipe_decoder.layers.12.layer_norms.2.bias\", \"recipe_decoder.layers.13.self_attn.in_proj_weight\", \"recipe_decoder.layers.13.self_attn.in_proj_bias\", \"recipe_decoder.layers.13.self_attn.out_proj.weight\", \"recipe_decoder.layers.13.self_attn.out_proj.bias\", \"recipe_decoder.layers.13.cond_att.in_proj_weight\", \"recipe_decoder.layers.13.cond_att.in_proj_bias\", \"recipe_decoder.layers.13.cond_att.out_proj.weight\", \"recipe_decoder.layers.13.cond_att.out_proj.bias\", \"recipe_decoder.layers.13.fc1.weight\", \"recipe_decoder.layers.13.fc1.bias\", \"recipe_decoder.layers.13.fc2.weight\", \"recipe_decoder.layers.13.fc2.bias\", \"recipe_decoder.layers.13.layer_norms.0.weight\", \"recipe_decoder.layers.13.layer_norms.0.bias\", \"recipe_decoder.layers.13.layer_norms.1.weight\", \"recipe_decoder.layers.13.layer_norms.1.bias\", \"recipe_decoder.layers.13.layer_norms.2.weight\", \"recipe_decoder.layers.13.layer_norms.2.bias\", \"recipe_decoder.layers.14.self_attn.in_proj_weight\", \"recipe_decoder.layers.14.self_attn.in_proj_bias\", \"recipe_decoder.layers.14.self_attn.out_proj.weight\", \"recipe_decoder.layers.14.self_attn.out_proj.bias\", \"recipe_decoder.layers.14.cond_att.in_proj_weight\", \"recipe_decoder.layers.14.cond_att.in_proj_bias\", \"recipe_decoder.layers.14.cond_att.out_proj.weight\", \"recipe_decoder.layers.14.cond_att.out_proj.bias\", \"recipe_decoder.layers.14.fc1.weight\", \"recipe_decoder.layers.14.fc1.bias\", \"recipe_decoder.layers.14.fc2.weight\", \"recipe_decoder.layers.14.fc2.bias\", \"recipe_decoder.layers.14.layer_norms.0.weight\", \"recipe_decoder.layers.14.layer_norms.0.bias\", \"recipe_decoder.layers.14.layer_norms.1.weight\", \"recipe_decoder.layers.14.layer_norms.1.bias\", \"recipe_decoder.layers.14.layer_norms.2.weight\", \"recipe_decoder.layers.14.layer_norms.2.bias\", \"recipe_decoder.layers.15.self_attn.in_proj_weight\", \"recipe_decoder.layers.15.self_attn.in_proj_bias\", \"recipe_decoder.layers.15.self_attn.out_proj.weight\", \"recipe_decoder.layers.15.self_attn.out_proj.bias\", \"recipe_decoder.layers.15.cond_att.in_proj_weight\", \"recipe_decoder.layers.15.cond_att.in_proj_bias\", \"recipe_decoder.layers.15.cond_att.out_proj.weight\", \"recipe_decoder.layers.15.cond_att.out_proj.bias\", \"recipe_decoder.layers.15.fc1.weight\", \"recipe_decoder.layers.15.fc1.bias\", \"recipe_decoder.layers.15.fc2.weight\", \"recipe_decoder.layers.15.fc2.bias\", \"recipe_decoder.layers.15.layer_norms.0.weight\", \"recipe_decoder.layers.15.layer_norms.0.bias\", \"recipe_decoder.layers.15.layer_norms.1.weight\", \"recipe_decoder.layers.15.layer_norms.1.bias\", \"recipe_decoder.layers.15.layer_norms.2.weight\", \"recipe_decoder.layers.15.layer_norms.2.bias\", \"recipe_decoder.linear.weight\", \"recipe_decoder.linear.bias\", \"image_encoder.vit.embeddings.cls_token\", \"image_encoder.vit.embeddings.position_embeddings\", \"image_encoder.vit.embeddings.patch_embeddings.projection.weight\", \"image_encoder.vit.embeddings.patch_embeddings.projection.bias\", \"image_encoder.vit.encoder.layer.0.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.0.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.0.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.0.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.0.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.0.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.0.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.0.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.0.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.0.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.0.output.dense.weight\", \"image_encoder.vit.encoder.layer.0.output.dense.bias\", \"image_encoder.vit.encoder.layer.0.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.0.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.0.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.0.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.1.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.1.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.1.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.1.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.1.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.1.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.1.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.1.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.1.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.1.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.1.output.dense.weight\", \"image_encoder.vit.encoder.layer.1.output.dense.bias\", \"image_encoder.vit.encoder.layer.1.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.1.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.1.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.1.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.2.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.2.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.2.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.2.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.2.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.2.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.2.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.2.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.2.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.2.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.2.output.dense.weight\", \"image_encoder.vit.encoder.layer.2.output.dense.bias\", \"image_encoder.vit.encoder.layer.2.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.2.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.2.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.2.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.3.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.3.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.3.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.3.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.3.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.3.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.3.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.3.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.3.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.3.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.3.output.dense.weight\", \"image_encoder.vit.encoder.layer.3.output.dense.bias\", \"image_encoder.vit.encoder.layer.3.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.3.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.3.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.3.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.4.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.4.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.4.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.4.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.4.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.4.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.4.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.4.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.4.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.4.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.4.output.dense.weight\", \"image_encoder.vit.encoder.layer.4.output.dense.bias\", \"image_encoder.vit.encoder.layer.4.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.4.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.4.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.4.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.5.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.5.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.5.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.5.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.5.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.5.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.5.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.5.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.5.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.5.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.5.output.dense.weight\", \"image_encoder.vit.encoder.layer.5.output.dense.bias\", \"image_encoder.vit.encoder.layer.5.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.5.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.5.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.5.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.6.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.6.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.6.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.6.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.6.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.6.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.6.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.6.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.6.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.6.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.6.output.dense.weight\", \"image_encoder.vit.encoder.layer.6.output.dense.bias\", \"image_encoder.vit.encoder.layer.6.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.6.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.6.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.6.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.7.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.7.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.7.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.7.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.7.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.7.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.7.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.7.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.7.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.7.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.7.output.dense.weight\", \"image_encoder.vit.encoder.layer.7.output.dense.bias\", \"image_encoder.vit.encoder.layer.7.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.7.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.7.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.7.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.8.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.8.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.8.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.8.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.8.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.8.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.8.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.8.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.8.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.8.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.8.output.dense.weight\", \"image_encoder.vit.encoder.layer.8.output.dense.bias\", \"image_encoder.vit.encoder.layer.8.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.8.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.8.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.8.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.9.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.9.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.9.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.9.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.9.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.9.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.9.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.9.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.9.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.9.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.9.output.dense.weight\", \"image_encoder.vit.encoder.layer.9.output.dense.bias\", \"image_encoder.vit.encoder.layer.9.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.9.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.9.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.9.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.10.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.10.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.10.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.10.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.10.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.10.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.10.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.10.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.10.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.10.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.10.output.dense.weight\", \"image_encoder.vit.encoder.layer.10.output.dense.bias\", \"image_encoder.vit.encoder.layer.10.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.10.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.10.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.10.layernorm_after.bias\", \"image_encoder.vit.encoder.layer.11.attention.attention.query.weight\", \"image_encoder.vit.encoder.layer.11.attention.attention.query.bias\", \"image_encoder.vit.encoder.layer.11.attention.attention.key.weight\", \"image_encoder.vit.encoder.layer.11.attention.attention.key.bias\", \"image_encoder.vit.encoder.layer.11.attention.attention.value.weight\", \"image_encoder.vit.encoder.layer.11.attention.attention.value.bias\", \"image_encoder.vit.encoder.layer.11.attention.output.dense.weight\", \"image_encoder.vit.encoder.layer.11.attention.output.dense.bias\", \"image_encoder.vit.encoder.layer.11.intermediate.dense.weight\", \"image_encoder.vit.encoder.layer.11.intermediate.dense.bias\", \"image_encoder.vit.encoder.layer.11.output.dense.weight\", \"image_encoder.vit.encoder.layer.11.output.dense.bias\", \"image_encoder.vit.encoder.layer.11.layernorm_before.weight\", \"image_encoder.vit.encoder.layer.11.layernorm_before.bias\", \"image_encoder.vit.encoder.layer.11.layernorm_after.weight\", \"image_encoder.vit.encoder.layer.11.layernorm_after.bias\", \"image_encoder.vit.layernorm.weight\", \"image_encoder.vit.layernorm.bias\", \"image_encoder.vit.pooler.dense.weight\", \"image_encoder.vit.pooler.dense.bias\", \"image_encoder.linear.weight\", \"image_encoder.linear.bias\", \"image_encoder.sequential.0.weight\", \"image_encoder.sequential.0.bias\", \"ingredient_decoder.embed_tokens.weight\", \"ingredient_decoder.layer_norms_in.0.weight\", \"ingredient_decoder.layer_norms_in.0.bias\", \"ingredient_decoder.layer_norms_in.1.weight\", \"ingredient_decoder.layer_norms_in.1.bias\", \"ingredient_decoder.layer_norms_in.2.weight\", \"ingredient_decoder.layer_norms_in.2.bias\", \"ingredient_decoder.layers.0.self_attn.in_proj_weight\", \"ingredient_decoder.layers.0.self_attn.in_proj_bias\", \"ingredient_decoder.layers.0.self_attn.out_proj.weight\", \"ingredient_decoder.layers.0.self_attn.out_proj.bias\", \"ingredient_decoder.layers.0.cond_att.in_proj_weight\", \"ingredient_decoder.layers.0.cond_att.in_proj_bias\", \"ingredient_decoder.layers.0.cond_att.out_proj.weight\", \"ingredient_decoder.layers.0.cond_att.out_proj.bias\", \"ingredient_decoder.layers.0.fc1.weight\", \"ingredient_decoder.layers.0.fc1.bias\", \"ingredient_decoder.layers.0.fc2.weight\", \"ingredient_decoder.layers.0.fc2.bias\", \"ingredient_decoder.layers.0.layer_norms.0.weight\", \"ingredient_decoder.layers.0.layer_norms.0.bias\", \"ingredient_decoder.layers.0.layer_norms.1.weight\", \"ingredient_decoder.layers.0.layer_norms.1.bias\", \"ingredient_decoder.layers.0.layer_norms.2.weight\", \"ingredient_decoder.layers.0.layer_norms.2.bias\", \"ingredient_decoder.layers.0.last_ln.weight\", \"ingredient_decoder.layers.0.last_ln.bias\", \"ingredient_decoder.layers.1.self_attn.in_proj_weight\", \"ingredient_decoder.layers.1.self_attn.in_proj_bias\", \"ingredient_decoder.layers.1.self_attn.out_proj.weight\", \"ingredient_decoder.layers.1.self_attn.out_proj.bias\", \"ingredient_decoder.layers.1.cond_att.in_proj_weight\", \"ingredient_decoder.layers.1.cond_att.in_proj_bias\", \"ingredient_decoder.layers.1.cond_att.out_proj.weight\", \"ingredient_decoder.layers.1.cond_att.out_proj.bias\", \"ingredient_decoder.layers.1.fc1.weight\", \"ingredient_decoder.layers.1.fc1.bias\", \"ingredient_decoder.layers.1.fc2.weight\", \"ingredient_decoder.layers.1.fc2.bias\", \"ingredient_decoder.layers.1.layer_norms.0.weight\", \"ingredient_decoder.layers.1.layer_norms.0.bias\", \"ingredient_decoder.layers.1.layer_norms.1.weight\", \"ingredient_decoder.layers.1.layer_norms.1.bias\", \"ingredient_decoder.layers.1.layer_norms.2.weight\", \"ingredient_decoder.layers.1.layer_norms.2.bias\", \"ingredient_decoder.layers.1.last_ln.weight\", \"ingredient_decoder.layers.1.last_ln.bias\", \"ingredient_decoder.layers.2.self_attn.in_proj_weight\", \"ingredient_decoder.layers.2.self_attn.in_proj_bias\", \"ingredient_decoder.layers.2.self_attn.out_proj.weight\", \"ingredient_decoder.layers.2.self_attn.out_proj.bias\", \"ingredient_decoder.layers.2.cond_att.in_proj_weight\", \"ingredient_decoder.layers.2.cond_att.in_proj_bias\", \"ingredient_decoder.layers.2.cond_att.out_proj.weight\", \"ingredient_decoder.layers.2.cond_att.out_proj.bias\", \"ingredient_decoder.layers.2.fc1.weight\", \"ingredient_decoder.layers.2.fc1.bias\", \"ingredient_decoder.layers.2.fc2.weight\", \"ingredient_decoder.layers.2.fc2.bias\", \"ingredient_decoder.layers.2.layer_norms.0.weight\", \"ingredient_decoder.layers.2.layer_norms.0.bias\", \"ingredient_decoder.layers.2.layer_norms.1.weight\", \"ingredient_decoder.layers.2.layer_norms.1.bias\", \"ingredient_decoder.layers.2.layer_norms.2.weight\", \"ingredient_decoder.layers.2.layer_norms.2.bias\", \"ingredient_decoder.layers.2.last_ln.weight\", \"ingredient_decoder.layers.2.last_ln.bias\", \"ingredient_decoder.layers.3.self_attn.in_proj_weight\", \"ingredient_decoder.layers.3.self_attn.in_proj_bias\", \"ingredient_decoder.layers.3.self_attn.out_proj.weight\", \"ingredient_decoder.layers.3.self_attn.out_proj.bias\", \"ingredient_decoder.layers.3.cond_att.in_proj_weight\", \"ingredient_decoder.layers.3.cond_att.in_proj_bias\", \"ingredient_decoder.layers.3.cond_att.out_proj.weight\", \"ingredient_decoder.layers.3.cond_att.out_proj.bias\", \"ingredient_decoder.layers.3.fc1.weight\", \"ingredient_decoder.layers.3.fc1.bias\", \"ingredient_decoder.layers.3.fc2.weight\", \"ingredient_decoder.layers.3.fc2.bias\", \"ingredient_decoder.layers.3.layer_norms.0.weight\", \"ingredient_decoder.layers.3.layer_norms.0.bias\", \"ingredient_decoder.layers.3.layer_norms.1.weight\", \"ingredient_decoder.layers.3.layer_norms.1.bias\", \"ingredient_decoder.layers.3.layer_norms.2.weight\", \"ingredient_decoder.layers.3.layer_norms.2.bias\", \"ingredient_decoder.layers.3.last_ln.weight\", \"ingredient_decoder.layers.3.last_ln.bias\", \"ingredient_decoder.linear.weight\", \"ingredient_decoder.linear.bias\". "
     ]
    }
   ],
   "source": [
    "model.image_encoder.load_state_dict(partial_vit_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['ingredient_encoder.linear.weight', 'recipe_decoder.embed_tokens.weight', 'recipe_decoder.embed_positions.weight', 'recipe_decoder.layers.0.self_attn.in_proj_weight', 'recipe_decoder.layers.0.self_attn.in_proj_bias', 'recipe_decoder.layers.0.self_attn.out_proj.weight', 'recipe_decoder.layers.0.self_attn.out_proj.bias', 'recipe_decoder.layers.0.cond_att.in_proj_weight', 'recipe_decoder.layers.0.cond_att.in_proj_bias', 'recipe_decoder.layers.0.cond_att.out_proj.weight', 'recipe_decoder.layers.0.cond_att.out_proj.bias', 'recipe_decoder.layers.0.fc1.weight', 'recipe_decoder.layers.0.fc1.bias', 'recipe_decoder.layers.0.fc2.weight', 'recipe_decoder.layers.0.fc2.bias', 'recipe_decoder.layers.0.layer_norms.0.weight', 'recipe_decoder.layers.0.layer_norms.0.bias', 'recipe_decoder.layers.0.layer_norms.1.weight', 'recipe_decoder.layers.0.layer_norms.1.bias', 'recipe_decoder.layers.0.layer_norms.2.weight', 'recipe_decoder.layers.0.layer_norms.2.bias', 'recipe_decoder.layers.1.self_attn.in_proj_weight', 'recipe_decoder.layers.1.self_attn.in_proj_bias', 'recipe_decoder.layers.1.self_attn.out_proj.weight', 'recipe_decoder.layers.1.self_attn.out_proj.bias', 'recipe_decoder.layers.1.cond_att.in_proj_weight', 'recipe_decoder.layers.1.cond_att.in_proj_bias', 'recipe_decoder.layers.1.cond_att.out_proj.weight', 'recipe_decoder.layers.1.cond_att.out_proj.bias', 'recipe_decoder.layers.1.fc1.weight', 'recipe_decoder.layers.1.fc1.bias', 'recipe_decoder.layers.1.fc2.weight', 'recipe_decoder.layers.1.fc2.bias', 'recipe_decoder.layers.1.layer_norms.0.weight', 'recipe_decoder.layers.1.layer_norms.0.bias', 'recipe_decoder.layers.1.layer_norms.1.weight', 'recipe_decoder.layers.1.layer_norms.1.bias', 'recipe_decoder.layers.1.layer_norms.2.weight', 'recipe_decoder.layers.1.layer_norms.2.bias', 'recipe_decoder.layers.2.self_attn.in_proj_weight', 'recipe_decoder.layers.2.self_attn.in_proj_bias', 'recipe_decoder.layers.2.self_attn.out_proj.weight', 'recipe_decoder.layers.2.self_attn.out_proj.bias', 'recipe_decoder.layers.2.cond_att.in_proj_weight', 'recipe_decoder.layers.2.cond_att.in_proj_bias', 'recipe_decoder.layers.2.cond_att.out_proj.weight', 'recipe_decoder.layers.2.cond_att.out_proj.bias', 'recipe_decoder.layers.2.fc1.weight', 'recipe_decoder.layers.2.fc1.bias', 'recipe_decoder.layers.2.fc2.weight', 'recipe_decoder.layers.2.fc2.bias', 'recipe_decoder.layers.2.layer_norms.0.weight', 'recipe_decoder.layers.2.layer_norms.0.bias', 'recipe_decoder.layers.2.layer_norms.1.weight', 'recipe_decoder.layers.2.layer_norms.1.bias', 'recipe_decoder.layers.2.layer_norms.2.weight', 'recipe_decoder.layers.2.layer_norms.2.bias', 'recipe_decoder.layers.3.self_attn.in_proj_weight', 'recipe_decoder.layers.3.self_attn.in_proj_bias', 'recipe_decoder.layers.3.self_attn.out_proj.weight', 'recipe_decoder.layers.3.self_attn.out_proj.bias', 'recipe_decoder.layers.3.cond_att.in_proj_weight', 'recipe_decoder.layers.3.cond_att.in_proj_bias', 'recipe_decoder.layers.3.cond_att.out_proj.weight', 'recipe_decoder.layers.3.cond_att.out_proj.bias', 'recipe_decoder.layers.3.fc1.weight', 'recipe_decoder.layers.3.fc1.bias', 'recipe_decoder.layers.3.fc2.weight', 'recipe_decoder.layers.3.fc2.bias', 'recipe_decoder.layers.3.layer_norms.0.weight', 'recipe_decoder.layers.3.layer_norms.0.bias', 'recipe_decoder.layers.3.layer_norms.1.weight', 'recipe_decoder.layers.3.layer_norms.1.bias', 'recipe_decoder.layers.3.layer_norms.2.weight', 'recipe_decoder.layers.3.layer_norms.2.bias', 'recipe_decoder.layers.4.self_attn.in_proj_weight', 'recipe_decoder.layers.4.self_attn.in_proj_bias', 'recipe_decoder.layers.4.self_attn.out_proj.weight', 'recipe_decoder.layers.4.self_attn.out_proj.bias', 'recipe_decoder.layers.4.cond_att.in_proj_weight', 'recipe_decoder.layers.4.cond_att.in_proj_bias', 'recipe_decoder.layers.4.cond_att.out_proj.weight', 'recipe_decoder.layers.4.cond_att.out_proj.bias', 'recipe_decoder.layers.4.fc1.weight', 'recipe_decoder.layers.4.fc1.bias', 'recipe_decoder.layers.4.fc2.weight', 'recipe_decoder.layers.4.fc2.bias', 'recipe_decoder.layers.4.layer_norms.0.weight', 'recipe_decoder.layers.4.layer_norms.0.bias', 'recipe_decoder.layers.4.layer_norms.1.weight', 'recipe_decoder.layers.4.layer_norms.1.bias', 'recipe_decoder.layers.4.layer_norms.2.weight', 'recipe_decoder.layers.4.layer_norms.2.bias', 'recipe_decoder.layers.5.self_attn.in_proj_weight', 'recipe_decoder.layers.5.self_attn.in_proj_bias', 'recipe_decoder.layers.5.self_attn.out_proj.weight', 'recipe_decoder.layers.5.self_attn.out_proj.bias', 'recipe_decoder.layers.5.cond_att.in_proj_weight', 'recipe_decoder.layers.5.cond_att.in_proj_bias', 'recipe_decoder.layers.5.cond_att.out_proj.weight', 'recipe_decoder.layers.5.cond_att.out_proj.bias', 'recipe_decoder.layers.5.fc1.weight', 'recipe_decoder.layers.5.fc1.bias', 'recipe_decoder.layers.5.fc2.weight', 'recipe_decoder.layers.5.fc2.bias', 'recipe_decoder.layers.5.layer_norms.0.weight', 'recipe_decoder.layers.5.layer_norms.0.bias', 'recipe_decoder.layers.5.layer_norms.1.weight', 'recipe_decoder.layers.5.layer_norms.1.bias', 'recipe_decoder.layers.5.layer_norms.2.weight', 'recipe_decoder.layers.5.layer_norms.2.bias', 'recipe_decoder.layers.6.self_attn.in_proj_weight', 'recipe_decoder.layers.6.self_attn.in_proj_bias', 'recipe_decoder.layers.6.self_attn.out_proj.weight', 'recipe_decoder.layers.6.self_attn.out_proj.bias', 'recipe_decoder.layers.6.cond_att.in_proj_weight', 'recipe_decoder.layers.6.cond_att.in_proj_bias', 'recipe_decoder.layers.6.cond_att.out_proj.weight', 'recipe_decoder.layers.6.cond_att.out_proj.bias', 'recipe_decoder.layers.6.fc1.weight', 'recipe_decoder.layers.6.fc1.bias', 'recipe_decoder.layers.6.fc2.weight', 'recipe_decoder.layers.6.fc2.bias', 'recipe_decoder.layers.6.layer_norms.0.weight', 'recipe_decoder.layers.6.layer_norms.0.bias', 'recipe_decoder.layers.6.layer_norms.1.weight', 'recipe_decoder.layers.6.layer_norms.1.bias', 'recipe_decoder.layers.6.layer_norms.2.weight', 'recipe_decoder.layers.6.layer_norms.2.bias', 'recipe_decoder.layers.7.self_attn.in_proj_weight', 'recipe_decoder.layers.7.self_attn.in_proj_bias', 'recipe_decoder.layers.7.self_attn.out_proj.weight', 'recipe_decoder.layers.7.self_attn.out_proj.bias', 'recipe_decoder.layers.7.cond_att.in_proj_weight', 'recipe_decoder.layers.7.cond_att.in_proj_bias', 'recipe_decoder.layers.7.cond_att.out_proj.weight', 'recipe_decoder.layers.7.cond_att.out_proj.bias', 'recipe_decoder.layers.7.fc1.weight', 'recipe_decoder.layers.7.fc1.bias', 'recipe_decoder.layers.7.fc2.weight', 'recipe_decoder.layers.7.fc2.bias', 'recipe_decoder.layers.7.layer_norms.0.weight', 'recipe_decoder.layers.7.layer_norms.0.bias', 'recipe_decoder.layers.7.layer_norms.1.weight', 'recipe_decoder.layers.7.layer_norms.1.bias', 'recipe_decoder.layers.7.layer_norms.2.weight', 'recipe_decoder.layers.7.layer_norms.2.bias', 'recipe_decoder.layers.8.self_attn.in_proj_weight', 'recipe_decoder.layers.8.self_attn.in_proj_bias', 'recipe_decoder.layers.8.self_attn.out_proj.weight', 'recipe_decoder.layers.8.self_attn.out_proj.bias', 'recipe_decoder.layers.8.cond_att.in_proj_weight', 'recipe_decoder.layers.8.cond_att.in_proj_bias', 'recipe_decoder.layers.8.cond_att.out_proj.weight', 'recipe_decoder.layers.8.cond_att.out_proj.bias', 'recipe_decoder.layers.8.fc1.weight', 'recipe_decoder.layers.8.fc1.bias', 'recipe_decoder.layers.8.fc2.weight', 'recipe_decoder.layers.8.fc2.bias', 'recipe_decoder.layers.8.layer_norms.0.weight', 'recipe_decoder.layers.8.layer_norms.0.bias', 'recipe_decoder.layers.8.layer_norms.1.weight', 'recipe_decoder.layers.8.layer_norms.1.bias', 'recipe_decoder.layers.8.layer_norms.2.weight', 'recipe_decoder.layers.8.layer_norms.2.bias', 'recipe_decoder.layers.9.self_attn.in_proj_weight', 'recipe_decoder.layers.9.self_attn.in_proj_bias', 'recipe_decoder.layers.9.self_attn.out_proj.weight', 'recipe_decoder.layers.9.self_attn.out_proj.bias', 'recipe_decoder.layers.9.cond_att.in_proj_weight', 'recipe_decoder.layers.9.cond_att.in_proj_bias', 'recipe_decoder.layers.9.cond_att.out_proj.weight', 'recipe_decoder.layers.9.cond_att.out_proj.bias', 'recipe_decoder.layers.9.fc1.weight', 'recipe_decoder.layers.9.fc1.bias', 'recipe_decoder.layers.9.fc2.weight', 'recipe_decoder.layers.9.fc2.bias', 'recipe_decoder.layers.9.layer_norms.0.weight', 'recipe_decoder.layers.9.layer_norms.0.bias', 'recipe_decoder.layers.9.layer_norms.1.weight', 'recipe_decoder.layers.9.layer_norms.1.bias', 'recipe_decoder.layers.9.layer_norms.2.weight', 'recipe_decoder.layers.9.layer_norms.2.bias', 'recipe_decoder.layers.10.self_attn.in_proj_weight', 'recipe_decoder.layers.10.self_attn.in_proj_bias', 'recipe_decoder.layers.10.self_attn.out_proj.weight', 'recipe_decoder.layers.10.self_attn.out_proj.bias', 'recipe_decoder.layers.10.cond_att.in_proj_weight', 'recipe_decoder.layers.10.cond_att.in_proj_bias', 'recipe_decoder.layers.10.cond_att.out_proj.weight', 'recipe_decoder.layers.10.cond_att.out_proj.bias', 'recipe_decoder.layers.10.fc1.weight', 'recipe_decoder.layers.10.fc1.bias', 'recipe_decoder.layers.10.fc2.weight', 'recipe_decoder.layers.10.fc2.bias', 'recipe_decoder.layers.10.layer_norms.0.weight', 'recipe_decoder.layers.10.layer_norms.0.bias', 'recipe_decoder.layers.10.layer_norms.1.weight', 'recipe_decoder.layers.10.layer_norms.1.bias', 'recipe_decoder.layers.10.layer_norms.2.weight', 'recipe_decoder.layers.10.layer_norms.2.bias', 'recipe_decoder.layers.11.self_attn.in_proj_weight', 'recipe_decoder.layers.11.self_attn.in_proj_bias', 'recipe_decoder.layers.11.self_attn.out_proj.weight', 'recipe_decoder.layers.11.self_attn.out_proj.bias', 'recipe_decoder.layers.11.cond_att.in_proj_weight', 'recipe_decoder.layers.11.cond_att.in_proj_bias', 'recipe_decoder.layers.11.cond_att.out_proj.weight', 'recipe_decoder.layers.11.cond_att.out_proj.bias', 'recipe_decoder.layers.11.fc1.weight', 'recipe_decoder.layers.11.fc1.bias', 'recipe_decoder.layers.11.fc2.weight', 'recipe_decoder.layers.11.fc2.bias', 'recipe_decoder.layers.11.layer_norms.0.weight', 'recipe_decoder.layers.11.layer_norms.0.bias', 'recipe_decoder.layers.11.layer_norms.1.weight', 'recipe_decoder.layers.11.layer_norms.1.bias', 'recipe_decoder.layers.11.layer_norms.2.weight', 'recipe_decoder.layers.11.layer_norms.2.bias', 'recipe_decoder.layers.12.self_attn.in_proj_weight', 'recipe_decoder.layers.12.self_attn.in_proj_bias', 'recipe_decoder.layers.12.self_attn.out_proj.weight', 'recipe_decoder.layers.12.self_attn.out_proj.bias', 'recipe_decoder.layers.12.cond_att.in_proj_weight', 'recipe_decoder.layers.12.cond_att.in_proj_bias', 'recipe_decoder.layers.12.cond_att.out_proj.weight', 'recipe_decoder.layers.12.cond_att.out_proj.bias', 'recipe_decoder.layers.12.fc1.weight', 'recipe_decoder.layers.12.fc1.bias', 'recipe_decoder.layers.12.fc2.weight', 'recipe_decoder.layers.12.fc2.bias', 'recipe_decoder.layers.12.layer_norms.0.weight', 'recipe_decoder.layers.12.layer_norms.0.bias', 'recipe_decoder.layers.12.layer_norms.1.weight', 'recipe_decoder.layers.12.layer_norms.1.bias', 'recipe_decoder.layers.12.layer_norms.2.weight', 'recipe_decoder.layers.12.layer_norms.2.bias', 'recipe_decoder.layers.13.self_attn.in_proj_weight', 'recipe_decoder.layers.13.self_attn.in_proj_bias', 'recipe_decoder.layers.13.self_attn.out_proj.weight', 'recipe_decoder.layers.13.self_attn.out_proj.bias', 'recipe_decoder.layers.13.cond_att.in_proj_weight', 'recipe_decoder.layers.13.cond_att.in_proj_bias', 'recipe_decoder.layers.13.cond_att.out_proj.weight', 'recipe_decoder.layers.13.cond_att.out_proj.bias', 'recipe_decoder.layers.13.fc1.weight', 'recipe_decoder.layers.13.fc1.bias', 'recipe_decoder.layers.13.fc2.weight', 'recipe_decoder.layers.13.fc2.bias', 'recipe_decoder.layers.13.layer_norms.0.weight', 'recipe_decoder.layers.13.layer_norms.0.bias', 'recipe_decoder.layers.13.layer_norms.1.weight', 'recipe_decoder.layers.13.layer_norms.1.bias', 'recipe_decoder.layers.13.layer_norms.2.weight', 'recipe_decoder.layers.13.layer_norms.2.bias', 'recipe_decoder.layers.14.self_attn.in_proj_weight', 'recipe_decoder.layers.14.self_attn.in_proj_bias', 'recipe_decoder.layers.14.self_attn.out_proj.weight', 'recipe_decoder.layers.14.self_attn.out_proj.bias', 'recipe_decoder.layers.14.cond_att.in_proj_weight', 'recipe_decoder.layers.14.cond_att.in_proj_bias', 'recipe_decoder.layers.14.cond_att.out_proj.weight', 'recipe_decoder.layers.14.cond_att.out_proj.bias', 'recipe_decoder.layers.14.fc1.weight', 'recipe_decoder.layers.14.fc1.bias', 'recipe_decoder.layers.14.fc2.weight', 'recipe_decoder.layers.14.fc2.bias', 'recipe_decoder.layers.14.layer_norms.0.weight', 'recipe_decoder.layers.14.layer_norms.0.bias', 'recipe_decoder.layers.14.layer_norms.1.weight', 'recipe_decoder.layers.14.layer_norms.1.bias', 'recipe_decoder.layers.14.layer_norms.2.weight', 'recipe_decoder.layers.14.layer_norms.2.bias', 'recipe_decoder.layers.15.self_attn.in_proj_weight', 'recipe_decoder.layers.15.self_attn.in_proj_bias', 'recipe_decoder.layers.15.self_attn.out_proj.weight', 'recipe_decoder.layers.15.self_attn.out_proj.bias', 'recipe_decoder.layers.15.cond_att.in_proj_weight', 'recipe_decoder.layers.15.cond_att.in_proj_bias', 'recipe_decoder.layers.15.cond_att.out_proj.weight', 'recipe_decoder.layers.15.cond_att.out_proj.bias', 'recipe_decoder.layers.15.fc1.weight', 'recipe_decoder.layers.15.fc1.bias', 'recipe_decoder.layers.15.fc2.weight', 'recipe_decoder.layers.15.fc2.bias', 'recipe_decoder.layers.15.layer_norms.0.weight', 'recipe_decoder.layers.15.layer_norms.0.bias', 'recipe_decoder.layers.15.layer_norms.1.weight', 'recipe_decoder.layers.15.layer_norms.1.bias', 'recipe_decoder.layers.15.layer_norms.2.weight', 'recipe_decoder.layers.15.layer_norms.2.bias', 'recipe_decoder.linear.weight', 'recipe_decoder.linear.bias', 'image_encoder.resnet.0.weight', 'image_encoder.resnet.1.weight', 'image_encoder.resnet.1.bias', 'image_encoder.resnet.1.running_mean', 'image_encoder.resnet.1.running_var', 'image_encoder.resnet.4.0.conv1.weight', 'image_encoder.resnet.4.0.bn1.weight', 'image_encoder.resnet.4.0.bn1.bias', 'image_encoder.resnet.4.0.bn1.running_mean', 'image_encoder.resnet.4.0.bn1.running_var', 'image_encoder.resnet.4.0.conv2.weight', 'image_encoder.resnet.4.0.bn2.weight', 'image_encoder.resnet.4.0.bn2.bias', 'image_encoder.resnet.4.0.bn2.running_mean', 'image_encoder.resnet.4.0.bn2.running_var', 'image_encoder.resnet.4.0.conv3.weight', 'image_encoder.resnet.4.0.bn3.weight', 'image_encoder.resnet.4.0.bn3.bias', 'image_encoder.resnet.4.0.bn3.running_mean', 'image_encoder.resnet.4.0.bn3.running_var', 'image_encoder.resnet.4.0.downsample.0.weight', 'image_encoder.resnet.4.0.downsample.1.weight', 'image_encoder.resnet.4.0.downsample.1.bias', 'image_encoder.resnet.4.0.downsample.1.running_mean', 'image_encoder.resnet.4.0.downsample.1.running_var', 'image_encoder.resnet.4.1.conv1.weight', 'image_encoder.resnet.4.1.bn1.weight', 'image_encoder.resnet.4.1.bn1.bias', 'image_encoder.resnet.4.1.bn1.running_mean', 'image_encoder.resnet.4.1.bn1.running_var', 'image_encoder.resnet.4.1.conv2.weight', 'image_encoder.resnet.4.1.bn2.weight', 'image_encoder.resnet.4.1.bn2.bias', 'image_encoder.resnet.4.1.bn2.running_mean', 'image_encoder.resnet.4.1.bn2.running_var', 'image_encoder.resnet.4.1.conv3.weight', 'image_encoder.resnet.4.1.bn3.weight', 'image_encoder.resnet.4.1.bn3.bias', 'image_encoder.resnet.4.1.bn3.running_mean', 'image_encoder.resnet.4.1.bn3.running_var', 'image_encoder.resnet.4.2.conv1.weight', 'image_encoder.resnet.4.2.bn1.weight', 'image_encoder.resnet.4.2.bn1.bias', 'image_encoder.resnet.4.2.bn1.running_mean', 'image_encoder.resnet.4.2.bn1.running_var', 'image_encoder.resnet.4.2.conv2.weight', 'image_encoder.resnet.4.2.bn2.weight', 'image_encoder.resnet.4.2.bn2.bias', 'image_encoder.resnet.4.2.bn2.running_mean', 'image_encoder.resnet.4.2.bn2.running_var', 'image_encoder.resnet.4.2.conv3.weight', 'image_encoder.resnet.4.2.bn3.weight', 'image_encoder.resnet.4.2.bn3.bias', 'image_encoder.resnet.4.2.bn3.running_mean', 'image_encoder.resnet.4.2.bn3.running_var', 'image_encoder.resnet.5.0.conv1.weight', 'image_encoder.resnet.5.0.bn1.weight', 'image_encoder.resnet.5.0.bn1.bias', 'image_encoder.resnet.5.0.bn1.running_mean', 'image_encoder.resnet.5.0.bn1.running_var', 'image_encoder.resnet.5.0.conv2.weight', 'image_encoder.resnet.5.0.bn2.weight', 'image_encoder.resnet.5.0.bn2.bias', 'image_encoder.resnet.5.0.bn2.running_mean', 'image_encoder.resnet.5.0.bn2.running_var', 'image_encoder.resnet.5.0.conv3.weight', 'image_encoder.resnet.5.0.bn3.weight', 'image_encoder.resnet.5.0.bn3.bias', 'image_encoder.resnet.5.0.bn3.running_mean', 'image_encoder.resnet.5.0.bn3.running_var', 'image_encoder.resnet.5.0.downsample.0.weight', 'image_encoder.resnet.5.0.downsample.1.weight', 'image_encoder.resnet.5.0.downsample.1.bias', 'image_encoder.resnet.5.0.downsample.1.running_mean', 'image_encoder.resnet.5.0.downsample.1.running_var', 'image_encoder.resnet.5.1.conv1.weight', 'image_encoder.resnet.5.1.bn1.weight', 'image_encoder.resnet.5.1.bn1.bias', 'image_encoder.resnet.5.1.bn1.running_mean', 'image_encoder.resnet.5.1.bn1.running_var', 'image_encoder.resnet.5.1.conv2.weight', 'image_encoder.resnet.5.1.bn2.weight', 'image_encoder.resnet.5.1.bn2.bias', 'image_encoder.resnet.5.1.bn2.running_mean', 'image_encoder.resnet.5.1.bn2.running_var', 'image_encoder.resnet.5.1.conv3.weight', 'image_encoder.resnet.5.1.bn3.weight', 'image_encoder.resnet.5.1.bn3.bias', 'image_encoder.resnet.5.1.bn3.running_mean', 'image_encoder.resnet.5.1.bn3.running_var', 'image_encoder.resnet.5.2.conv1.weight', 'image_encoder.resnet.5.2.bn1.weight', 'image_encoder.resnet.5.2.bn1.bias', 'image_encoder.resnet.5.2.bn1.running_mean', 'image_encoder.resnet.5.2.bn1.running_var', 'image_encoder.resnet.5.2.conv2.weight', 'image_encoder.resnet.5.2.bn2.weight', 'image_encoder.resnet.5.2.bn2.bias', 'image_encoder.resnet.5.2.bn2.running_mean', 'image_encoder.resnet.5.2.bn2.running_var', 'image_encoder.resnet.5.2.conv3.weight', 'image_encoder.resnet.5.2.bn3.weight', 'image_encoder.resnet.5.2.bn3.bias', 'image_encoder.resnet.5.2.bn3.running_mean', 'image_encoder.resnet.5.2.bn3.running_var', 'image_encoder.resnet.5.3.conv1.weight', 'image_encoder.resnet.5.3.bn1.weight', 'image_encoder.resnet.5.3.bn1.bias', 'image_encoder.resnet.5.3.bn1.running_mean', 'image_encoder.resnet.5.3.bn1.running_var', 'image_encoder.resnet.5.3.conv2.weight', 'image_encoder.resnet.5.3.bn2.weight', 'image_encoder.resnet.5.3.bn2.bias', 'image_encoder.resnet.5.3.bn2.running_mean', 'image_encoder.resnet.5.3.bn2.running_var', 'image_encoder.resnet.5.3.conv3.weight', 'image_encoder.resnet.5.3.bn3.weight', 'image_encoder.resnet.5.3.bn3.bias', 'image_encoder.resnet.5.3.bn3.running_mean', 'image_encoder.resnet.5.3.bn3.running_var', 'image_encoder.resnet.6.0.conv1.weight', 'image_encoder.resnet.6.0.bn1.weight', 'image_encoder.resnet.6.0.bn1.bias', 'image_encoder.resnet.6.0.bn1.running_mean', 'image_encoder.resnet.6.0.bn1.running_var', 'image_encoder.resnet.6.0.conv2.weight', 'image_encoder.resnet.6.0.bn2.weight', 'image_encoder.resnet.6.0.bn2.bias', 'image_encoder.resnet.6.0.bn2.running_mean', 'image_encoder.resnet.6.0.bn2.running_var', 'image_encoder.resnet.6.0.conv3.weight', 'image_encoder.resnet.6.0.bn3.weight', 'image_encoder.resnet.6.0.bn3.bias', 'image_encoder.resnet.6.0.bn3.running_mean', 'image_encoder.resnet.6.0.bn3.running_var', 'image_encoder.resnet.6.0.downsample.0.weight', 'image_encoder.resnet.6.0.downsample.1.weight', 'image_encoder.resnet.6.0.downsample.1.bias', 'image_encoder.resnet.6.0.downsample.1.running_mean', 'image_encoder.resnet.6.0.downsample.1.running_var', 'image_encoder.resnet.6.1.conv1.weight', 'image_encoder.resnet.6.1.bn1.weight', 'image_encoder.resnet.6.1.bn1.bias', 'image_encoder.resnet.6.1.bn1.running_mean', 'image_encoder.resnet.6.1.bn1.running_var', 'image_encoder.resnet.6.1.conv2.weight', 'image_encoder.resnet.6.1.bn2.weight', 'image_encoder.resnet.6.1.bn2.bias', 'image_encoder.resnet.6.1.bn2.running_mean', 'image_encoder.resnet.6.1.bn2.running_var', 'image_encoder.resnet.6.1.conv3.weight', 'image_encoder.resnet.6.1.bn3.weight', 'image_encoder.resnet.6.1.bn3.bias', 'image_encoder.resnet.6.1.bn3.running_mean', 'image_encoder.resnet.6.1.bn3.running_var', 'image_encoder.resnet.6.2.conv1.weight', 'image_encoder.resnet.6.2.bn1.weight', 'image_encoder.resnet.6.2.bn1.bias', 'image_encoder.resnet.6.2.bn1.running_mean', 'image_encoder.resnet.6.2.bn1.running_var', 'image_encoder.resnet.6.2.conv2.weight', 'image_encoder.resnet.6.2.bn2.weight', 'image_encoder.resnet.6.2.bn2.bias', 'image_encoder.resnet.6.2.bn2.running_mean', 'image_encoder.resnet.6.2.bn2.running_var', 'image_encoder.resnet.6.2.conv3.weight', 'image_encoder.resnet.6.2.bn3.weight', 'image_encoder.resnet.6.2.bn3.bias', 'image_encoder.resnet.6.2.bn3.running_mean', 'image_encoder.resnet.6.2.bn3.running_var', 'image_encoder.resnet.6.3.conv1.weight', 'image_encoder.resnet.6.3.bn1.weight', 'image_encoder.resnet.6.3.bn1.bias', 'image_encoder.resnet.6.3.bn1.running_mean', 'image_encoder.resnet.6.3.bn1.running_var', 'image_encoder.resnet.6.3.conv2.weight', 'image_encoder.resnet.6.3.bn2.weight', 'image_encoder.resnet.6.3.bn2.bias', 'image_encoder.resnet.6.3.bn2.running_mean', 'image_encoder.resnet.6.3.bn2.running_var', 'image_encoder.resnet.6.3.conv3.weight', 'image_encoder.resnet.6.3.bn3.weight', 'image_encoder.resnet.6.3.bn3.bias', 'image_encoder.resnet.6.3.bn3.running_mean', 'image_encoder.resnet.6.3.bn3.running_var', 'image_encoder.resnet.6.4.conv1.weight', 'image_encoder.resnet.6.4.bn1.weight', 'image_encoder.resnet.6.4.bn1.bias', 'image_encoder.resnet.6.4.bn1.running_mean', 'image_encoder.resnet.6.4.bn1.running_var', 'image_encoder.resnet.6.4.conv2.weight', 'image_encoder.resnet.6.4.bn2.weight', 'image_encoder.resnet.6.4.bn2.bias', 'image_encoder.resnet.6.4.bn2.running_mean', 'image_encoder.resnet.6.4.bn2.running_var', 'image_encoder.resnet.6.4.conv3.weight', 'image_encoder.resnet.6.4.bn3.weight', 'image_encoder.resnet.6.4.bn3.bias', 'image_encoder.resnet.6.4.bn3.running_mean', 'image_encoder.resnet.6.4.bn3.running_var', 'image_encoder.resnet.6.5.conv1.weight', 'image_encoder.resnet.6.5.bn1.weight', 'image_encoder.resnet.6.5.bn1.bias', 'image_encoder.resnet.6.5.bn1.running_mean', 'image_encoder.resnet.6.5.bn1.running_var', 'image_encoder.resnet.6.5.conv2.weight', 'image_encoder.resnet.6.5.bn2.weight', 'image_encoder.resnet.6.5.bn2.bias', 'image_encoder.resnet.6.5.bn2.running_mean', 'image_encoder.resnet.6.5.bn2.running_var', 'image_encoder.resnet.6.5.conv3.weight', 'image_encoder.resnet.6.5.bn3.weight', 'image_encoder.resnet.6.5.bn3.bias', 'image_encoder.resnet.6.5.bn3.running_mean', 'image_encoder.resnet.6.5.bn3.running_var', 'image_encoder.resnet.7.0.conv1.weight', 'image_encoder.resnet.7.0.bn1.weight', 'image_encoder.resnet.7.0.bn1.bias', 'image_encoder.resnet.7.0.bn1.running_mean', 'image_encoder.resnet.7.0.bn1.running_var', 'image_encoder.resnet.7.0.conv2.weight', 'image_encoder.resnet.7.0.bn2.weight', 'image_encoder.resnet.7.0.bn2.bias', 'image_encoder.resnet.7.0.bn2.running_mean', 'image_encoder.resnet.7.0.bn2.running_var', 'image_encoder.resnet.7.0.conv3.weight', 'image_encoder.resnet.7.0.bn3.weight', 'image_encoder.resnet.7.0.bn3.bias', 'image_encoder.resnet.7.0.bn3.running_mean', 'image_encoder.resnet.7.0.bn3.running_var', 'image_encoder.resnet.7.0.downsample.0.weight', 'image_encoder.resnet.7.0.downsample.1.weight', 'image_encoder.resnet.7.0.downsample.1.bias', 'image_encoder.resnet.7.0.downsample.1.running_mean', 'image_encoder.resnet.7.0.downsample.1.running_var', 'image_encoder.resnet.7.1.conv1.weight', 'image_encoder.resnet.7.1.bn1.weight', 'image_encoder.resnet.7.1.bn1.bias', 'image_encoder.resnet.7.1.bn1.running_mean', 'image_encoder.resnet.7.1.bn1.running_var', 'image_encoder.resnet.7.1.conv2.weight', 'image_encoder.resnet.7.1.bn2.weight', 'image_encoder.resnet.7.1.bn2.bias', 'image_encoder.resnet.7.1.bn2.running_mean', 'image_encoder.resnet.7.1.bn2.running_var', 'image_encoder.resnet.7.1.conv3.weight', 'image_encoder.resnet.7.1.bn3.weight', 'image_encoder.resnet.7.1.bn3.bias', 'image_encoder.resnet.7.1.bn3.running_mean', 'image_encoder.resnet.7.1.bn3.running_var', 'image_encoder.resnet.7.2.conv1.weight', 'image_encoder.resnet.7.2.bn1.weight', 'image_encoder.resnet.7.2.bn1.bias', 'image_encoder.resnet.7.2.bn1.running_mean', 'image_encoder.resnet.7.2.bn1.running_var', 'image_encoder.resnet.7.2.conv2.weight', 'image_encoder.resnet.7.2.bn2.weight', 'image_encoder.resnet.7.2.bn2.bias', 'image_encoder.resnet.7.2.bn2.running_mean', 'image_encoder.resnet.7.2.bn2.running_var', 'image_encoder.resnet.7.2.conv3.weight', 'image_encoder.resnet.7.2.bn3.weight', 'image_encoder.resnet.7.2.bn3.bias', 'image_encoder.resnet.7.2.bn3.running_mean', 'image_encoder.resnet.7.2.bn3.running_var', 'image_encoder.linear.0.weight', 'image_encoder.linear.0.bias', 'ingredient_decoder.embed_tokens.weight', 'ingredient_decoder.layer_norms_in.0.weight', 'ingredient_decoder.layer_norms_in.0.bias', 'ingredient_decoder.layer_norms_in.1.weight', 'ingredient_decoder.layer_norms_in.1.bias', 'ingredient_decoder.layer_norms_in.2.weight', 'ingredient_decoder.layer_norms_in.2.bias', 'ingredient_decoder.layers.0.self_attn.in_proj_weight', 'ingredient_decoder.layers.0.self_attn.in_proj_bias', 'ingredient_decoder.layers.0.self_attn.out_proj.weight', 'ingredient_decoder.layers.0.self_attn.out_proj.bias', 'ingredient_decoder.layers.0.cond_att.in_proj_weight', 'ingredient_decoder.layers.0.cond_att.in_proj_bias', 'ingredient_decoder.layers.0.cond_att.out_proj.weight', 'ingredient_decoder.layers.0.cond_att.out_proj.bias', 'ingredient_decoder.layers.0.fc1.weight', 'ingredient_decoder.layers.0.fc1.bias', 'ingredient_decoder.layers.0.fc2.weight', 'ingredient_decoder.layers.0.fc2.bias', 'ingredient_decoder.layers.0.layer_norms.0.weight', 'ingredient_decoder.layers.0.layer_norms.0.bias', 'ingredient_decoder.layers.0.layer_norms.1.weight', 'ingredient_decoder.layers.0.layer_norms.1.bias', 'ingredient_decoder.layers.0.layer_norms.2.weight', 'ingredient_decoder.layers.0.layer_norms.2.bias', 'ingredient_decoder.layers.0.last_ln.weight', 'ingredient_decoder.layers.0.last_ln.bias', 'ingredient_decoder.layers.1.self_attn.in_proj_weight', 'ingredient_decoder.layers.1.self_attn.in_proj_bias', 'ingredient_decoder.layers.1.self_attn.out_proj.weight', 'ingredient_decoder.layers.1.self_attn.out_proj.bias', 'ingredient_decoder.layers.1.cond_att.in_proj_weight', 'ingredient_decoder.layers.1.cond_att.in_proj_bias', 'ingredient_decoder.layers.1.cond_att.out_proj.weight', 'ingredient_decoder.layers.1.cond_att.out_proj.bias', 'ingredient_decoder.layers.1.fc1.weight', 'ingredient_decoder.layers.1.fc1.bias', 'ingredient_decoder.layers.1.fc2.weight', 'ingredient_decoder.layers.1.fc2.bias', 'ingredient_decoder.layers.1.layer_norms.0.weight', 'ingredient_decoder.layers.1.layer_norms.0.bias', 'ingredient_decoder.layers.1.layer_norms.1.weight', 'ingredient_decoder.layers.1.layer_norms.1.bias', 'ingredient_decoder.layers.1.layer_norms.2.weight', 'ingredient_decoder.layers.1.layer_norms.2.bias', 'ingredient_decoder.layers.1.last_ln.weight', 'ingredient_decoder.layers.1.last_ln.bias', 'ingredient_decoder.layers.2.self_attn.in_proj_weight', 'ingredient_decoder.layers.2.self_attn.in_proj_bias', 'ingredient_decoder.layers.2.self_attn.out_proj.weight', 'ingredient_decoder.layers.2.self_attn.out_proj.bias', 'ingredient_decoder.layers.2.cond_att.in_proj_weight', 'ingredient_decoder.layers.2.cond_att.in_proj_bias', 'ingredient_decoder.layers.2.cond_att.out_proj.weight', 'ingredient_decoder.layers.2.cond_att.out_proj.bias', 'ingredient_decoder.layers.2.fc1.weight', 'ingredient_decoder.layers.2.fc1.bias', 'ingredient_decoder.layers.2.fc2.weight', 'ingredient_decoder.layers.2.fc2.bias', 'ingredient_decoder.layers.2.layer_norms.0.weight', 'ingredient_decoder.layers.2.layer_norms.0.bias', 'ingredient_decoder.layers.2.layer_norms.1.weight', 'ingredient_decoder.layers.2.layer_norms.1.bias', 'ingredient_decoder.layers.2.layer_norms.2.weight', 'ingredient_decoder.layers.2.layer_norms.2.bias', 'ingredient_decoder.layers.2.last_ln.weight', 'ingredient_decoder.layers.2.last_ln.bias', 'ingredient_decoder.layers.3.self_attn.in_proj_weight', 'ingredient_decoder.layers.3.self_attn.in_proj_bias', 'ingredient_decoder.layers.3.self_attn.out_proj.weight', 'ingredient_decoder.layers.3.self_attn.out_proj.bias', 'ingredient_decoder.layers.3.cond_att.in_proj_weight', 'ingredient_decoder.layers.3.cond_att.in_proj_bias', 'ingredient_decoder.layers.3.cond_att.out_proj.weight', 'ingredient_decoder.layers.3.cond_att.out_proj.bias', 'ingredient_decoder.layers.3.fc1.weight', 'ingredient_decoder.layers.3.fc1.bias', 'ingredient_decoder.layers.3.fc2.weight', 'ingredient_decoder.layers.3.fc2.bias', 'ingredient_decoder.layers.3.layer_norms.0.weight', 'ingredient_decoder.layers.3.layer_norms.0.bias', 'ingredient_decoder.layers.3.layer_norms.1.weight', 'ingredient_decoder.layers.3.layer_norms.1.bias', 'ingredient_decoder.layers.3.layer_norms.2.weight', 'ingredient_decoder.layers.3.layer_norms.2.bias', 'ingredient_decoder.layers.3.last_ln.weight', 'ingredient_decoder.layers.3.last_ln.bias', 'ingredient_decoder.linear.weight', 'ingredient_decoder.linear.bias'])\n"
     ]
    }
   ],
   "source": [
    "recipe_model_path = '/data/prateek/github/see-food/paper_weights/modelbest.ckpt'\n",
    "partial_recipe_state_dict = torch.load(recipe_model_path, map_location=torch.device('cpu'))\n",
    "print(partial_recipe_state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderVisionTransformer(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=768, out_features=49, bias=True)\n",
       "  (sequential): Sequential(\n",
       "    (0): Conv2d(197, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): Dropout2d(p=0.3, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_state_dict = {'0.weight': partial_state_dict['0.weight'],\n",
    "                     '0.bias': partial_state_dict['0.bias']}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
